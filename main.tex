% Created 2025-07-03 Thu 17:13
% Intended LaTeX compiler: pdflatex
\documentclass[10pt]{book}
%% CREATO CON ORG - EMACS
\newcommand{\use}[2][]{\usepackage[#1]{#2}}
% PACCHETTI FONDAMENTLAI
\use[utf8]{inputenc}
\use[T1]{fontenc}
\use{graphicx}
\use{longtable}
\use{wrapfig}
\use{rotating}
\use[normalem]{ulem}
\use{amsmath}
\use{amsthm}
\use{amssymb}
\use{capt-of}
\use[italian]{babel}
\use[babel]{csquotes}
\use[style=numeric, hyperref]{biblatex}
\use{microtype}
\use{lmodern}
\use{subfig} % sottofigure
\use{multicol} % due colonne
\use{lipsum} % lorem ipsum
\use{color} % colori in latex
\use{parskip} % rimuove l'indentazione dei nuovi paragrafi %% Add parbox=false to all new tcolorbox
\use{centernot}
\use[outline]{contour}\contourlength{3pt}
\use{fancyhdr}
\use{layout}
\use[most]{tcolorbox} % Riquadri colorati
\use{ifthen} % IFTHEN
\use{geometry}

% pacchetti matematica
\use{yhmath}
\use{dsfont}
\use{mathrsfs}
\use{cancel} % semplificare
\use{polynom} %divisione tra polinomi
\use{forest} % grafi ad albero
\use{booktabs} % tabelle
\use{commath} %simboli e differenziali
\use{bm} %bold
\use[fulladjust]{marginnote} %to use marginnote for date notes
\use{arrayjobx}%array
\use[intlimits]{empheq} % Riquadri colorati attorno alle equazioni
\use{mathtools}
\use{circuitikz} % Disegnare i circuiti

%%%%%%%%%%%%%


%%%% QUIVER
\newcommand{\duepunti}{\,\mathchar\numexpr"6000+`:\relax\,}
% A TikZ style for curved arrows of a fixed height, due to AndréC.
\tikzset{curve/.style={settings={#1},to path={(\tikztostart)
    .. controls ($(\tikztostart)!\pv{pos}!(\tikztotarget)!\pv{height}!270:(\tikztotarget)$)
    and ($(\tikztostart)!1-\pv{pos}!(\tikztotarget)!\pv{height}!270:(\tikztotarget)$)
    .. (\tikztotarget)\tikztonodes}},
    settings/.code={\tikzset{quiver/.cd,#1}
        \def\pv##1{\pgfkeysvalueof{/tikz/quiver/##1}}},
    quiver/.cd,pos/.initial=0.35,height/.initial=0}

% TikZ arrowhead/tail styles.
\tikzset{tail reversed/.code={\pgfsetarrowsstart{tikzcd to}}}
\tikzset{2tail/.code={\pgfsetarrowsstart{Implies[reversed]}}}
\tikzset{2tail reversed/.code={\pgfsetarrowsstart{Implies}}}
% TikZ arrow styles.
\tikzset{no body/.style={/tikz/dash pattern=on 0 off 1mm}}
%%%%%%%%%%


%% DEFINIZIONI COMANDI MATEMATICI
\let\sin\relax %TOGLIE LA DEFINIZIONE SU "\sin"

% cambia la definizione di empty set
% ---
\let\oldemptyset\emptyset
% ---
% \let\emptyset\varnothing
% ---
% \let\emptyset\relax
% \newcommand{\emptyset}{\text{\textnormal{\O}}}
% ---

\DeclareMathOperator{\bounded}{bd}
\DeclareMathOperator{\sin}{sen}
\DeclareMathOperator{\epi}{Epi}
\DeclareMathOperator{\cl}{cl}
\DeclareMathOperator{\graph}{graph}
\DeclareMathOperator{\arcsec}{arcsec}
\DeclareMathOperator{\arccot}{arccot}
\DeclareMathOperator{\arccsc}{arccsc}
\DeclareMathOperator{\spettro}{Spettro}
\DeclareMathOperator{\nulls}{nullspace}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\ar}{ar}
\DeclareMathOperator{\const}{Const}
\DeclareMathOperator{\fun}{Fun}
\DeclareMathOperator{\rel}{Rel}
\DeclareMathOperator{\altezza}{ht}
\let\det\relax %TOGLIE LA DEFINIZIONE SU "\det"
\DeclareMathOperator{\det}{det}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\gl}{GL}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\id}{Id}
\DeclareMathOperator{\I}{\mathds{1}}
\DeclareMathOperator{\II}{II}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\tc}{t.c.}
\DeclareMathOperator{\T}{T}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\st}{st}
\DeclareMathOperator{\mon}{Mon}
\newcommand{\card}[1]{\left\vert #1 \right\vert}
\newcommand{\trasposta}[1]{\prescript{\text{T}}{}{#1}}
\newcommand{\1}{\mathds{1}}
\newcommand{\R}{\mathds{R}}
\newcommand{\diesis}{\#}
\newcommand{\bemolle}{\flat}
\newcommand{\nonstandard}[1]{\prescript{*}{}{#1}}
\newcommand{\starR}{\nonstandard{\R}}
\newcommand{\borel}{\mathscr{B}}
\newcommand{\lebesgue}[1]{\mathscr{L}\left(#1\right)}
\newcommand{\media}{\mathds{E}}
\newcommand{\K}{\mathds{K}}
\newcommand{\A}{\mathds{A}}
\newcommand{\Q}{\mathds{Q}}
\newcommand{\N}{\mathds{N}}
\newcommand{\C}{\mathds{C}}
\newcommand{\Z}{\mathds{Z}}
\newcommand{\qo}{\hspace{1em}\text{q.o.}\,}
\renewcommand{\tilde}[1]{\widetilde{#1}}
\renewcommand{\parallel}{\mathrel{/\mkern-5mu/}}
\newcommand{\parti}[2][]{\wp_{#1}(#2)}
\newcommand{\diff}[1]{\operatorname{d}_{#1}}
\let\oldvec\vec
\renewcommand{\vec}[1]{\overrightarrow{\vphantom{i}#1}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\cat}[1]{\mathbf{#1}}
\newcommand{\dfreccia}[1]{\xrightarrow{\ #1 \ }}
\newcommand{\sfreccia}[1]{\xleftarrow{\ #1 \ }}
\newcommand{\formalsum}[2]{{\sum_{#1}^{#2}}{\vphantom{\sum}}'}
\newcommand{\minim}[2]{\mu_{#1}\, \left(#2\right)}
\newcommand{\concat}{\null^{\frown}} % concatenazione di stringe
\newcommand{\godelcode}[1]{\langle\!\langle #1 \rangle\!\rangle}
\newcommand{\godeldec}[1]{(\!(#1)\!)}
\newcommand{\termcode}[1]{\ulcorner #1\urcorner}
\newcommand{\partialto}{\dashrightarrow}
\newcommand{\restricted}{\upharpoonright}
\newcommand{\embeds}{\precsim}
\newcommand{\surjects}{\twoheadrightarrow}
\newcommand{\equipotenti}{\asymp}
%% \newcommand{\dotplus}{\mathbin{\dot{+}}} %% A quanto pare esiste già
\newcommand{\bigdot}{\mathbin{\boldsymbol{\cdot}}}
\newcommand{\dotexp}[1]{^{.#1}}
\newcommand{\conv}{\mathbin{*}}
\newcommand{\convolution}[2]{(#1\conv #2)}

%% Definizione di \dotminus

\makeatletter
\newcommand{\dotminus}{\mathbin{\text{\@dotminus}}}

\newcommand{\@dotminus}{%
  \ooalign{\hidewidth\raise1ex\hbox{.}\hidewidth\cr$\m@th-$\cr}%
}
\makeatother

%tramite i prossimi due comandi posso decidere come scrivere i logaritmi naturali in tutti i documenti: ho infatti eliminato qualsiasi differenza tra "ln" e "log": se si vuole qualcosa di diverso bisogna inserire manualmente il tutto
\let\ln\relax
\DeclareMathOperator{\ln}{ln}
\let\log\relax
\DeclareMathOperator{\log}{log}
%%%%%%

%% NUOVI COMANDI
\newcommand{\straniero}[1]{\textit{#1}} %parole straniere
\newcommand{\titolo}[1]{\textsc{#1}} %titoli
\newcommand{\qedd}{\tag*{$\blacksquare$}} %qed per ambienti matemastici
\renewcommand{\qedsymbol}{$\blacksquare$} %modifica colore qed
\newcommand{\ooverline}[1]{\overline{\overline{#1}}}
\newcommand{\circoletto}[1]{\left(#1\right)^{\text{o}}}
%
\newcommand{\qmatrice}[1]{\begin{pmatrix}
#1_{11} & \cdots & #1_{1n}\\
\vdots & \ddots & \vdots \\
#1_{m1} & \cdots & #1_{mn}
\end{pmatrix}}
%
\newcommand{\parentesi}[2]{%
\underset{#1}{\underbrace{#2}}%
}
%
\newcommand{\norma}[1]{% Norma
\left\lVert#1\right\rVert%
}
\newcommand{\scalare}[2]{% Scalare
\left\langle #1, #2\right\rangle
}
%%%%%

%% RESTRIZIONI
\newcommand{\referenze}[2]{
	\phantomsection{}#2\textsuperscript{\textcolor{blue}{\textbf{#1}}}
}

\let\restriction\relax

\def\restriction#1#2{\mathchoice
              {\setbox1\hbox{${\displaystyle #1}_{\scriptstyle #2}$}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{${\textstyle #1}_{\scriptstyle #2}$}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{${\scriptstyle #1}_{\scriptscriptstyle #2}$}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{${\scriptscriptstyle #1}_{\scriptscriptstyle #2}$}
              \restrictionaux{#1}{#2}}}
\def\restrictionaux#1#2{{#1\,\smash{\vrule height .8\ht1 depth .85\dp1}}_{\,#2}}
%%%%%%%%%%%

%% SEZIONE GRAFICA
\use{tikz}
\usetikzlibrary{matrix, patterns, calc, decorations.pathreplacing, hobby, decorations.markings, decorations.pathmorphing, babel}
\use{tikz-3dplot}
\use{mathrsfs} %per geogebra
\use{tikz-cd}
\tikzset
{
  %surface/.style={fill=black!10, shading=ball,fill opacity=0.4},
  plane/.style={black,pattern=north east lines},
  curve/.style={black,line width=0.5mm},
  dritto/.style={decoration={markings,mark=at position 0.5 with {\arrow{Stealth}}}, postaction=decorate},
  rovescio/.style={decoration={markings,mark=at position 0.5 with {\arrow{Stealth[reversed]}}}, postaction=decorate}
}
\use{pgfplots} % stampare le funzioni
	\pgfplotsset{/pgf/number format/use comma,compat=1.15}
	%\pgfplotsset{compat=1.15} %per geogebra
	\usepgfplotslibrary{fillbetween, polar}
%%%%%%

%% CITAZIONI
\use{lineno}

\newcommand{\citazione}[1]{%
  \begin{quotation}
  \begin{linenumbers}
  \modulolinenumbers[5]
  \begingroup
  \setlength{\parindent}{0cm}
  \noindent #1
  \endgroup
  \end{linenumbers}
  \end{quotation}\setcounter{linenumber}{1}
  }
%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% AMS THM

\theoremstyle{definition}% default
\newtheorem{thm}{Teorema}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposizione}
\newtheorem{cor}[thm]{Corollario}
\newtheorem{esempio}[thm]{Esempio}
\theoremstyle{plain}
\newtheorem{definizione}[thm]{Definizione}
\theoremstyle{remark}
\newtheorem*{oss}{Osservazione}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\use{hyperref}
\hypersetup{%
	pdfauthor={Davide Peccioli},
	pdfsubject={},
	allcolors=black,
	citecolor=black,
%	colorlinks=true,
	bookmarksopen=true}
\pagestyle{empty}

\renewcommand{\href}[2]{\textcolor{blue}{#2}}
\author{Davide Peccioli}
\date{a.a. 2024-25}
\title{Metodi Matematici per il Machine Learning}
\begin{document}

\maketitle
\tableofcontents

\part{De Rossi}
\chapter{Reti Neurali}
\label{sec:org009ba68}

\section{Neurone Artificiale}
\label{sec:org3008588}
Un \uline{neurone} è una cellula del corpo umano che può essere schematizzata come segue:
\begin{quote}
A neuron is a cell which consists of the following parts: dendrites, axon,and body-cell. The synapse is the connection between the axon of one neuron and the dendrite of another. The functions of each part is briefly described below:
\begin{itemize}
\item Dendrites are transmission channels that collect information from the axons of other neurons. The signal traveling through an axon reaches its terminal end and produces some chemicals \(x_{i}\) which are liberated in the synaptic gap. These chemicals are acting on the dendrites of the next neuron either in a strong or a weak way. The connection strength is described by the weight system \(w_{i}\)-
\item The body-cell collects all signals from dendrites. Here the dendrites activity adds up into a total potential and if a certain threshold is reached, the neuron fires a signal through the axon. The threshold depends on the sensitivity of the neuron and measures how easy is to get the neuron to fire.
\item The axon is the channel for signal propagation. The signal consists in the movement of ions from the body-cell towards the end of the axon. The signal is transmitted electrochemically to the dendrites of the next neuron.
\end{itemize}
\end{quote}

Matematicamente, quindi, si considera un neurone come una unità che riceve degli input (un vettore \(\bm{x}\)), lo \href{../../../../../org/roam/20250625095723-prodotto_scalare.org}{moltiplica} per un vettore di pesi \(\bm{w} = (w_{0},\dots,w_{n})\), somma un certo \uline{bias}, e produce un output processando il prodotto scalare tramite una \uline{\hyperref[sec:orgeed12bd]{funzione di attivazione}}:
\begin{equation*}
\begin{tikzcd}[ampersand replacement=\&,cramped]
	{\text{Input: }\bm{x}} \\
	{\bm{x}\cdot\bm{w}} \\
	{\text{Funzione di attivazione}} \\
	{\text{Output: }y}
	\arrow[from=1-1, to=2-1]
	\arrow[from=2-1, to=3-1]
	\arrow[from=3-1, to=4-1]
\end{tikzcd}
\end{equation*}
\subsection{Neurone Sigma-Heaviside}
\label{sec:orgcc2e5be}

Il modello più semplice è quello che riceve degli input, li somma dopo averli moltiplicati per dei pesi, e:
\begin{itemize}
\item restituisce \(0\) se la somma così ottenuta non supera un \uline{treshold} \(b\);
\item restituisce \(1\) se la somma così ottenuta è maggiore o uguale a \(b\).
\end{itemize}

Questo viene schematizzato in questo modo:
\begin{equation*}
\begin{tikzcd}[ampersand replacement=\&,cramped]
	{-1} \\
	{x_1} \\
	{x_2} \&\& {\boxed{\Sigma, H}} \&\& y \\
	\vdots \\
	{x_n}
	\arrow["b"{description}, from=1-1, to=3-3]
	\arrow["{w_1}"{description}, from=2-1, to=3-3]
	\arrow["{w_2}"{description}, from=3-1, to=3-3]
	\arrow[from=3-3, to=3-5]
	\arrow["{w_n}"{description}, from=5-1, to=3-3]
\end{tikzcd}
\end{equation*}
e l'output \(y\) è dato da\footnote{La funzione \(H:\R\to \R\) è la \href{../../../../../org/roam/20250624161413-funzione_di_heaviside.org}{Funzione di Heaviside}:
\begin{equation*}
H(x) = \begin{cases}
1 & x\ge 0\\
0 & x<0
\end{cases}
\end{equation*}}
\begin{equation*}
y=H\left(\sum_{i=0}^{n} x_{i}\,w_{i}\right)
\end{equation*}
dove per convenzione si è posto \(w_{0}=b\) e \(x_{0}=-1\).

La convenzione è per semplicità di notazione, infatti
\begin{equation*}
H\left(\sum_{i=0}^{n} x_{i}\,w_{i}\right)=\begin{cases}
1 & \sum_{i=1}^{n} x_{i}\, w_{i}\ge b\\
0 & \sum_{i=1}^{n} x_{i}\, w_{i}< b
\end{cases}
\end{equation*}
\subsection{Regressione Lineare}
\label{sec:orgc1f9431}

Un altro esempio di neurone è quello che approssima\footnote{Approssima in senso \(L^{2}\) (ovvero minimizza la \href{../../../../../org/roam/20250624162220-spazi_lp.org}{norma \(L^{2}\)} della differenza).} una \href{../../../../../org/roam/20250103103252-funzione_continua.org}{funzione continua}
\begin{equation*}
f: K\to \R
\end{equation*}
con \(K \subseteq \R^{n}\) \href{../../../../../org/roam/20250103163701-spazio_topologico_compatto.org}{compatto}.

L'input del neurone sarà una \href{../../../../../org/roam/20250206170922-sequenze_e_stringhe.org}{\(n\)-upla} \(X=(x_{1},\dots,x_{n}) \in K\), mentre l'output sarà la funzione lineare
\begin{equation*}
L(X) = b+\sum_{i=1}^{n} a_{i}\,x_{i}
\end{equation*}

Per semplicità si considera l'approssimazione vicino allo zero, e si suppone che
\begin{equation*}
L(0)=f(0)=0
\end{equation*}
(a meno di traslazione verticale per \(f(0)\)).

Si vuole quindi minimizzare
\begin{equation*}
C(a_{1},\dots,a_{n}) = \frac{1}{2}\norma{f-L}_{L^{2}} = \frac{1}{2}\int_{K} \left(\sum_{i=1}^{n} a_{i}x_{i} - f(X)\right)^{2}\dif x_{1}\cdots\dif x_{n}
\end{equation*}
calcolandone il \href{../../../../../org/roam/20250624171244-gradiente_di_una_funzione.org}{gradiente}
\begin{align*}
\dpd{C}{a_{k}} &= \int_{K} x_{k} \left(\sum_{i=1}^{n} a_{i}x_{i} - f(X)\right)\dif x_{1}\cdots\dif x_{n}\\
&= \sum_{i=1}^{n} a_{i} \int_{K} x_{i} x_{k}\dif x_{1}\cdots\dif x_{n} - \int_{K} x_{k} f(X)\dif x_{1}\cdots\dif x_{n}
\end{align*}
e dunque, posti
\begin{equation*}
\rho_{ij} \coloneqq \int_{K} x_{i}x_{j}\dif x_{1}\cdots\dif x_{n},\qquad m_{k} \coloneqq \int_{K} x_{k}f(X)\dif x_{1}\cdots\dif x_{n}
\end{equation*}
si ha che
\begin{equation*}
\dpd{C}{a_{k}} = \sum_{i=1}^{n}a_{i}\,\rho_{ik} - m_{k}
\end{equation*}
ovvero, in forma matriciale, posta\footnote{Vedi:
\begin{itemize}
\item \href{../../../../../org/roam/20250104111539-spazio_delle_matrici.org}{Spazio delle matrici}
\item \href{../../../../../org/roam/20250113144338-matrice_trasposta.org}{Matrice Trasposta}
\item \href{../../../../../org/roam/20250624171244-gradiente_di_una_funzione.org}{Gradiente di una funzione}
\end{itemize}} \(\rho=(\rho_{ij})\), \(\bm{a} = \null^{T}(a_{1},\dots,a_{n})\) e \(\bm{m} = \null^{T}(m_{1},\dots,m_{n})\):
\begin{equation*}
\nabla C = \rho\, \bm{a} - \bm{m}
\end{equation*}

Dunque, posto che \(\rho\) sia \href{../../../../../org/roam/20250104111735-matrice_invertibile.org}{invertibile}, si ottiene che i valori ottimali per \(L\) siano
\begin{equation*}
\bm{a} = \rho^{-1}\bm{m}.
\end{equation*}

Nel caso di funzioni a valori in \(\R^{m}\) il problema si scompone nelle diverse coordinate.
\section{Funzioni di attivazione}
\label{sec:orgeed12bd}
Nel Machine Learning le funzioni che agiscono nei \hyperref[sec:org3008588]{neuroni} vengono dette \uline{funzioni di attivazione}. Se ne presentano alcuni esempi, con i loro nomi specifici.

Sono tutte funzioni \(A \subseteq\R\to B \subseteq \R\).
\subsection{Funzioni Lineari}
\label{sec:orge8e59b9}

Tra le funzioni di attivazione utilizzate vi sono le seguenti funzioni lineari:
\begin{itemize}
\item \(f(x) = kx\), per \(k > 0\) costante;
\item la funzione identità \(x\mapsto x\).
\end{itemize}
\subsection{Step Functions}
\label{sec:org0c5ed4a}

\subsubsection{Threshold step function}
\label{sec:orgfb29ea7}

La \href{../../../../../org/roam/20250624161413-funzione_di_heaviside.org}{funzione di Heaviside} (vedi Fig. \ref{fig:heav})
\begin{equation*}
H(x)=\begin{cases}
1 & x\ge 0\\
0 & x<0
\end{cases}
\end{equation*}
la cui derivata (nel senso delle \href{../../../../../org/roam/20250625100117-distribuzione_analisi_matematica.org}{distribuzioni}) è una \href{../../../../../org/roam/20250625100133-delta_di_dirac.org}{Delta di Dirac}: \(H'(x) = \delta(x)\):
\begin{equation*}
\delta(x) = \begin{cases}
0 & x\neq 0\\
+\infty & x=0
\end{cases}
\end{equation*}
\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[axis lines=middle, axis equal]
\addplot[domain=-2:0, samples=100, ultra thick, red] {0};
\addplot[domain=0:2, samples=100, ultra thick, red] {1};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{La funzione di Heaviside}\label{fig:heav}
\end{figure}
\subsubsection{Bipolar step function}
\label{sec:org62f6c14}

La funzione segno: (vedi Fig. \ref{fig:bip})
\begin{equation*}
S(x)= \begin{cases}
1 & x\ge{0}\\
-1 & x<0
\end{cases}
\end{equation*}
per cui vale: \(S(x)=2H(x)-1\). Pertanto la sua derivata è
\begin{equation*}
S'(x) = 2H'(x)=2\delta(x)
\end{equation*}
\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[axis lines=middle, axis equal]
\addplot[domain=-2:0, samples=100, ultra thick, red] {-1};
\addplot[domain=0:2, samples=100, ultra thick, red] {1};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{La funzione segno}\label{fig:bip}
\end{figure}
\subsection{Hockeystick Functions}
\label{sec:org04a8bfe}

\subsubsection{Funzione di attivazione ReLU}
\label{sec:org8e4299d}
La \emph{Rectified Linear Unit} (ReLU) è (vedi Fig. \ref{fig:relu})
\begin{equation*}
\operatorname{ReLU}(x) = xH(x) = \max\set{0,x}
\end{equation*}
e la sua derivata \(\operatorname{ReLU}'(x) = H(x)\).

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[axis lines=middle, axis equal]
\addplot[domain=-2:0, samples=100, ultra thick, red] {0};
\addplot[domain=0:2, samples=100, ultra thick, red] {x};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{La funzione \(\operatorname{RELU}(x)\)}\label{fig:relu}
\end{figure}
\subsubsection{PReLU}
\label{sec:orgf12c96e}

La \emph{Parametric Rectivied Linear Unit} (PReLU) è (vedi Fig \ref{fig:prelu}), per \(\alpha>0\)
\begin{equation*}
\operatorname{PReLU}(\alpha;x) = \operatorname{PReLU}_{\alpha}(x) = \begin{cases}
\alpha x & x<0\\
x & x \ge 0
\end{cases}
\end{equation*}

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[axis lines=middle, axis equal]
\addplot[domain=-2:0, samples=100, ultra thick, red] {x};
\addplot[domain=0:2, samples=100, ultra thick, red] {2 * x};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{La funzione \(\operatorname{PRELU}_{2}(x)\)}\label{fig:prelu}
\end{figure}
\subsubsection{ELU}
\label{sec:org403e0b7}

La \emph{Exponential Linear Units} (ELU) è (vedi Fig. \ref{fig:elu}):
\begin{equation*}
\operatorname{ELU}(\alpha,x) = \begin{cases}
x &x>0\\
\alpha(e^{x}-1) &x\le 0
\end{cases}
\end{equation*}

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[axis lines=middle, axis equal]
\addplot[domain=-2:0, samples=100, ultra thick, red] {x};
\addplot[domain=0:2, samples=100, ultra thick, red] {2 * exp(x) - 2};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{La funzione \(\operatorname{ELU}(\alpha,x)\)}\label{fig:elu}
\end{figure}
\subsubsection{SELU}
\label{sec:org8500ea1}

La \emph{Scaled Exponential Linear Units} (SELU) è (vedi Fig. \ref{fig:selu})
\begin{equation*}
\operatorname{SELU}(\alpha,\lambda,x) = \lambda\operatorname{ELU}(\alpha,x) = \begin{cases}
\lambda\,x & x>0\\
\alpha\lambda(e^{x}-1) &x\le 0.
\end{cases}
\end{equation*}

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[axis lines=middle, axis equal]
\addplot[domain=-2:0, samples=100, ultra thick, red] {0.4 * x};
\addplot[domain=0:2, samples=100, ultra thick, red] {0.8 * exp(x) - 0.8};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{La funzione \(\operatorname{SELU}(0.4,2,x)\)}\label{fig:selu}
\end{figure}
\subsubsection{SLU}
\label{sec:orgbb2c55d}

La \emph{Sigmoid Linear Units} (SLU) è (vedi Fig. \ref{fig:slu})
\begin{equation*}
\phi(x) = \frac{x}{1+e^{-x}}.
\end{equation*}

Questa non è una \href{../../../../../org/roam/20250203132953-funzione_monotona.org}{funzione monotona}, ma ha un \href{../../../../../org/roam/20250627153543-massimo_e_minimo_di_una_funzione_reale.org}{minimo} in \(x_{0}\approx -1,27\).

Spesso si usa anche la versione parametrica:
\begin{equation*}
\phi_{c}(x) = \frac{x}{1+e^{-cx}}.
\end{equation*}

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[axis lines=middle, axis equal]
\addplot[domain=-7:7, samples=100, ultra thick, red] {x / (1 + exp(-x))};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{La funzione \(\operatorname{SLU}(x)\)}\label{fig:slu}
\end{figure}
\subsubsection{Funzione Softplus}
\label{sec:orga531b0f}
Questa è una funzione positiva crescente, con \href{../../../../../org/roam/20250202173528-dominio_range_e_campo_di_una_classe_relazione.org}{range} \((0,+\infty)\): (vedi Fig. \ref{fig:sp})
\begin{equation*}
\operatorname{sp}(x) = \ln(1+e^{x}).
\end{equation*}

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[axis lines=middle, axis equal]
\addplot[domain=-3:3, samples=100, ultra thick, red] {ln(1 + exp(x))};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{La funzione \(\operatorname{sp}(x)\)}\label{fig:sp}
\end{figure}

Inoltre
\begin{align*}
\operatorname{sp}(x)- \operatorname{sp}(-x) &= \ln(1+e^{x}) - \ln(1+e^{-x}) \\
&= \ln\left(\frac{1+e^{x}}{1+e^{-x}}\right) = \ln\left(\frac{1+e^{x}}{e^{-x}(1+e^{x})}\right) \\
&= \ln e^{x} = x.
\end{align*}
La sua derivata è
\begin{equation*}
\operatorname{sp}'(x) = \frac{1}{1+e^{-x}}>0.
\end{equation*}
\subsection{Funzioni Sigmoidali}
\label{sec:orgda90175}

\subsubsection{Funzione Logistica}
\label{sec:org1d40102}
La funzione logistica è (vedi Fig. \ref{fig:logistic})
\begin{equation*}
\sigma_{c}(x) = \sigma(c;x) = \frac{1}{1+e^{-cx}}.
\end{equation*}

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[axis lines=middle, axis equal]
\addplot[domain=-3:3, samples=100, ultra thick, red] {1 / (1 + exp(- 2 * x))};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{La funzione \(\sigma_{2}(x)\)}\label{fig:logistic}
\end{figure}

La famiglia di funzioni \((\sigma_{c}(x))_{c \in (0,+\infty)}\) approssima la \href{../../../../../org/roam/20250624161413-funzione_di_heaviside.org}{funzione di Heaviside} \(H(x)\), in quanto
\begin{equation*}
\lim_{c\to \infty} e^{-cx} = \begin{cases}
0 & x>0\\
1 & x=0\\
+ \infty & x<0
\end{cases}
\end{equation*}
e pertanto
\begin{equation*}
\lim_{c\to+\infty} \sigma_{c}(x) = \begin{cases}
1 &x>0\\
\frac{1}{2} & x=0\\
0 &x<0
\end{cases}
\end{equation*}
e pertanto, per ogni \(x\neq 0\): \(H(x) =\lim_{c\to+\infty}\sigma_{c}(x)\).

Le funzioni logistiche sono soluzioni della seguente equazione differenziale:
\begin{equation*}
\sigma_{c}' = c\sigma_{c}\,(1-\sigma_{c})
\end{equation*}
infatti:
\begin{align*}
\sigma_{c}'(x) &= - \frac{1}{(1+e^{-cx})^{2}} \cdot (-c\,e^{-cx})\\
&= c \cdot \frac{1}{1+e^{-cx}} \cdot \frac{e^{-cx}}{1+e^{-cx}}\\
&= c \cdot \frac{1}{1+e^{-cx}} \cdot \left(\frac{e^{-cx}+1-1}{1+e^{-cx}}\right)\\
&= c \cdot \frac{1}{1+e^{-cx}} \cdot \left(1+\frac{-1}{1+e^{-cx}}\right)\\
&= c\cdot \sigma_{c}(x) \cdot (1-\sigma_{c}(x)).
\end{align*}

Spesso ci si rifesce a \(\sigma\coloneqq \sigma_{1}\) come alla \uline{funzione logistica}.
\subsubsection{Tangente Iperbolica}
\label{sec:org5209844}

La \uline{\href{../../../../../org/roam/20250627184228-funzioni_iperboliche.org}{tangente iperbolica}} \(\tanh(x)\) è (vedi Fig. \ref{fig:tanh})
\begin{equation*}
\tanh(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} = 2\sigma_{2}(x)-1
\end{equation*}

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[axis lines=middle, axis equal]
\addplot[domain=-2:2, samples=100, ultra thick, red] {(exp(x) - exp(-x)) / (exp(x) + exp(-x))};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{La funzione \(\tanh(x)\)}\label{fig:tanh}
\end{figure}

Inoltre si ha che \(\tanh'(x) = 1-(\tanh(x))^{2}\):
\begin{align*}
\tanh'(x) &= 2\sigma_{2}'(x) = 2\cdot 2\sigma_{2}(x)\cdot(1-\sigma_{2}(x))\\
&= 2\sigma_{2}(x) \cdot (2-2\sigma_{2}(x))\\
&= (\tanh(x)+1) \cdot (1 - 2\sigma_{2}(x)+1)\\
&= (\tanh(x)+1)(-\tanh(x)+1) = 1-(\tanh(x))^{2}
\end{align*}
\subsubsection{Arcotangente}
\label{sec:orgbc86833}

È spesso utilizzata la seguente \href{../../../../../org/roam/20250627184319-funzioni_trigonometriche.org}{arcotangente} (vedi Fig. \ref{fig:arctan}):
\begin{equation*}
h(x) = \frac{2}{\pi} \arctan(x) \qquad x \in \R.
\end{equation*}

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[axis lines=middle, axis equal]
\addplot[domain=-2:2, samples=100, ultra thick, red] {rad(atan(x)) * 2 / 3.1415};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{La funzione \(h(x)\)}\label{fig:arctan}
\end{figure}
\subsubsection{Softsign}
\label{sec:orgddae878}

La seguente funzione differenziabile è la funzione \emph{softsign}: (vedi Fig. \ref{fig:softsign})
\begin{equation*}
\operatorname{so}(x)=\frac{x}{1+|x|},\qquad x \in \R
\end{equation*}
che ha raange \((-1,1)\).

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[axis lines=middle, axis equal]
\addplot[domain=-2:2, samples=100, ultra thick, red] {x / (1 + abs(x))};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{La funzione \(\operatorname{so}(x)\)\)}\label{fig:softsign}
\end{figure}
\subsubsection{Piecewise Linear}
\label{sec:orga919fd6}

Dato un parametro \(\alpha>0\) (vedi Fig. \ref{fig:pieclin})
\begin{equation*}
f_{\alpha}(x) = f(\alpha,x) = \begin{cases}
-1 & x\le-\alpha\\
x/\alpha & -\alpha<x<\alpha\\
1 &x\ge \alpha.
\end{cases}
\end{equation*}

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[axis lines=middle, axis equal]
\addplot[domain=-4:-2, samples=100, ultra thick, red] {-1};
\addplot[domain=-2:2, samples=100, ultra thick, red] {x / 2};
\addplot[domain=2:4, samples=100, ultra thick, red] {1};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{La funzione \(f_{2}(x)\)}\label{fig:pieclin}
\end{figure}
\subsection{Bumped-type Functions}
\label{sec:orgfe3de1f}

\subsubsection{Gaussiana}
\label{sec:orga2dbab7}

La funzione gaussiana mappa \(\R\) nell'intervallo \((0,1]\): (vedi Fig. \ref{fig:gauss})
\begin{equation*}
g(x) = e^{-x^{2}},\qquad x \in \R.
\end{equation*}

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[axis lines=middle, axis equal]
\addplot[domain=-4:4, samples=100, ultra thick, red] {exp(- x * x)};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{La funzione \(g(x)\)}\label{fig:gauss}
\end{figure}
\subsubsection{Doppio esponenziale}
\label{sec:org65125f4}

Mappa la retta reale nell'intervallo \((0,1]\) ed è definita: (vedi Fig. \ref{fig:dexp})
\begin{equation*}
f(x) = e^{-\lambda\,|x|},\qquad x \in \R, \lambda>0.
\end{equation*}

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[axis lines=middle, axis equal]
\addplot[domain=-4:0.01, samples=100, ultra thick, red] {exp(2 * x)};
\addplot[domain=-0.01:4, samples=100, ultra thick, red] {exp(- 2 * x)};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{La funzione \(f(x)\) con parametro \(\lambda=2\)}\label{fig:dexp}
\end{figure}
\section{Rete Neurale}
\label{sec:org18ff892}
Una \uline{rete neurale} è [\ldots{}]

Questa riceve degli \uline{input} e produce un \uline{output}, in base a certi parametri \(\bm{w}\), per simulare la \uline{FUNZIONE TARGET}; quest'ultima è l'obiettivo finale della Rete Neurale (ovvero si vuole far sì che l'output della rete neurale sia il più vicino possibile al risultato della funzione target).

Per misurare la distanza tra l'output di una rete e la funzione target si utilizza una \uline{funzione errore} (o \uline{funzione costo}), che deve essere scelta in base all'applicazione specifica.

Il \uline{\hyperref[sec:org320b895]{processo di apprendimento}} è quello che, partendo dai parametri \(\bm{w}\), li modifica (iterativamente), fino a dei parametri \(\bm{w}^{*}\), che sono \uline{ottimali}, nel senso che minimizzano la \uline{funzione errore}. Dunque il processo di apprendimento comporta la minimizzazione della funzione costo.
\subsection{Hidden Layer di una rete neurale}
\label{sec:orgf3e3b34}
\section{Funzioni costo (Machine Learning)}
\label{sec:org95f85ae}
Una funzione costo è una funzione che misura, dati certi parametri \(\bm{w}\) di una rete neurale, \uline{quanto la rete neurale si discosta dalla funzione target}.
\subsection{La Funzione Errore Supremum}
\label{sec:org44925e1}

Una rete neurale prende input \(x \in [0,1]\) e deve imparare una data funzione continua \(\phi:[0,1]\to [0,1]\).

La funzione della rete neurale, dipendete dai parametri \(\bm{w},b\), è \(f_{\bm{w},b}(x)\).

La funzione costo Supremum è
\begin{equation*}
C(\bm{w},b) \coloneqq \sup_{x \in [0,1]}|f_{\bm{w},b}(x)-\phi(x)|.
\end{equation*}

Se la funzione \(\phi\) è conosciuta solo per \(N\) valori \(x_{1},\dots,x_{N}\), allora la funzione costo diventa
\begin{equation*}
C(\bm{w},b) \coloneqq \max_{i=1,\dots, N} |f_{\bm{w},b}(x_{i})-\phi(x_{i})|.
\end{equation*}
\subsection{La Funzione Errore Norma L2}
\label{sec:org27d8d6f}

Una rete neurale prende input \(x \in [0,1]\) e deve imparare una data funzione \(\phi:[0,1]\to \R\) tale che
\begin{equation*}
\int_{0}^{1}(\phi(x))^{2}\dif x <\infty
\end{equation*}

La funzione della rete neurale, dipendete dai parametri \(\bm{w},\bm{b}\), è \(f_{\bm{w},\bm{b}}(x)\). La funzione costo associata a questo tipo di problema è quella che misura la distanza nella \href{../../../../../org/roam/20250625123506-spazio_normato.org}{norma} \href{../../../../../org/roam/20250624162220-spazi_lp.org}{\(L^{2}\)}:
\begin{equation*}
C(\bm{w},\bm{b}) \coloneqq \int_{[0,1]} (f_{\bm{w},\bm{b}}(x)-\phi(x))^{2}\dif x.
\end{equation*}

Se la funzione \(\phi\) è conosciuta soltanto in \(N\) punti
\begin{equation*}
z_{1}=\phi(x_{1}),\quad z_{2}=\phi(x_{2}),\qquad, z_{N} = \phi(x_{N})
\end{equation*}
allora, posti \(\bm{z}=(z_{1},\dots,z_{N})\) e \(\bm{x} = (x_{1},\dots,x_{N})\), la funzione costo diventa la \href{../../../../../org/roam/20250301193511-spazio_metrico.org}{distanza} in \(\R^{N}\) tra \(\bm{z}\) e \(f_{\bm{w},\bm{b}}(\bm{x}) \coloneqq \left(f_{\bm{w},\bm{b}}(z_{1}),\dots,f_{\bm{w},\bm{b}}(z_{N})\right)\):
\begin{equation*}
C(\bm{w},\bm{b}) = \norma{\bm{z}-f_{\bm{w},\bm{b}}(\bm{x})}^{2} = \sum_{i=1}^{N} |z_{i}-f_{\bm{w},\bm{b}}(x_{i})|^{2}
\end{equation*}
\subsubsection{Interpretazione Geometrica}
\label{sec:org4325b6e}

Fissati \(\bm{x}\) e \(\bm{z}\), la mappa \((\bm{w},\bm{b})\mapsto f_{\bm{w},\bm{b}}(\bm{x})\) rappresenta una ipersuperficie in \(\R^{N}\):
\begin{equation*}
\Phi(\bm{w},\bm{b}) = \begin{pmatrix}
\Phi_{1}(\bm{w},\bm{b})\\
\vdots\\
\Phi_{N}(\bm{w},\bm{b})\\
\end{pmatrix} = \begin{pmatrix}
f_{\bm{w},\bm{b}}(x_{1})\\
\vdots\\
f_{\bm{w},\bm{b}}(x_{N})
\end{pmatrix}
\end{equation*}
e la funzione costo \(C(\bm{w},\bm{b})\) è la \href{../../../../../org/roam/20250301193511-spazio_metrico.org}{distanza} euclidea in \(\R^{N}\) di un punto sulla ipersuperficie dal punto \(\bm{z}\). Si suppongano appropriate ipotesi di differenziabilità della ipersuperficie.

Il costo è minimizzato in \((\bm{w}^{*},\bm{b}^{*})\) quando la distanza è minima, ovvero quando \(\Phi(\bm{w}^{*},\bm{b}^{*})\) è la proiezione ortogonale di \(\bm{z}\) sulla ipersuperficie: questo significa che il vettore \(\Phi(\bm{w}^{*},\bm{b}^{*})-\bm{z}\) è ortogonale al \href{../../../../../org/roam/20250114102823-spazio_tangente_ad_un_punto_di_una_varieta_differenziabile.org}{piano tangente} alla ipersuperficie in \(\Phi(\bm{w}^{*},\bm{b}^{*})\): quest'ultimo è generato dai vettori\footnote{Vedi: ``\href{../../../../../org/roam/20250114103236-derivata_parziale.org}{Derivata parziale}''}
\begin{equation*}
\restriction{\partial_{w_{k}} \Phi(\bm{w},\bm{b})}{(\bm{w}^{*},\bm{b}^{*})}; \qquad \restriction{\partial_{b_{j}} \Phi(\bm{w},\bm{b})}{(\bm{w}^{*},\bm{b}^{*})}
\end{equation*}

Richiedere l'ortogonalità, quindi, significa richiedere che i prodotti scalari:
\begin{align*}
\left(\restriction{\partial_{w_{k}} \Phi(\bm{w},\bm{b})}{(\bm{w}^{*},\bm{b}^{*})}\right)\cdot (\Phi(\bm{w}^{*},\bm{b}^{*})-\bm{z}) &= 0\\
\left(\restriction{\partial_{b_{j}} \Phi(\bm{w},\bm{b})}{(\bm{w}^{*},\bm{b}^{*})}\right) \cdot(\Phi(\bm{w}^{*},\bm{b}^{*})-\bm{z}) &= 0.
\end{align*}

Queste sono le \uline{equazioni normali}, che operativamente diventano
\begin{align*}
\sum_{i=1}^{N} (f_{\bm{w},\bm{b}}(x_{i})-z_{i})\cdot \restriction{\partial_{w_{k}} f_{\bm{w},\bm{b}} (x_{i})}{(\bm{w},\bm{b}) = (\bm{w}^{*},\bm{b}^{*})} &= 0\\
\sum_{i=1}^{N} (f_{\bm{w},\bm{b}}(x_{i})-z_{i})\cdot \restriction{\partial_{b_{j}} f_{\bm{w},\bm{b}} (x_{i})}{(\bm{w},\bm{b}) = (\bm{w}^{*},\bm{b}^{*})} &= 0
\end{align*}
\subsection{Regolarizzazione della Funzione Costo (Machine Learning)}
\label{sec:org2dcb0b4}
Per evitare il fenomeno dell'\href{../../../../../org/roam/20250627103519-overfitting.org}{overfitting}, è bene mantenere i parametri \uline{piccoli}. Pertanto, data una \hyperref[sec:org95f85ae]{funzione costo} \(C(\bm{w})\), la si \uline{regolarizza}, utilizzando una funzione costo \(G(\bm{w})\), data da \(C(\bm{w})\) più un termine di regolarizzazione.

\begin{description}
\item[{Regolarizzazione \(L^{2}\).}] Si aggiunge alla funzione \(C(\bm{w})\) la \href{../../../../../org/roam/20250627104832-p_norma_in_rn.org}{2-norma} in \(\R^{n}\) dei parametri:
\begin{equation*}
  G(\bm{w}) \coloneqq C(\bm{w}) + \lambda\norma{\bm{w}}^{2}_{2},\qquad\text{dove }\norma{\bm{w}}^{2}_{2} = \sum_{i=1}^{n} (w_{i})^{2}.
\end{equation*}
Il valore \(\lambda>0\) è un \href{../../../../../org/roam/20250627104011-moltiplicatore_di_lagrange.org}{moltiplicatore di Lagrange}; questo parametro deve essere scelto in maniera da minimizzare l'\emph{overfitting}.
\item[{Regolarizzazione \(L^{1}\).}] Si aggiunge alla funzione \(C(\bm{w})\) la 1-norma in \(\R^{n}\) dei parametri:
\begin{equation*}
  G(\bm{w}) \coloneqq C(\bm{w}) + \lambda\norma{\bm{w}}^{2}_{2},\qquad\text{dove }\norma{\bm{w}}_{1} = \sum_{i=1}^{n} |w_{i}|.
\end{equation*}
Il valore \(\lambda>0\) è un \href{../../../../../org/roam/20250627104011-moltiplicatore_di_lagrange.org}{moltiplicatore di Lagrange}. Questo metodo, non differenziabile nell'origine, potrebbe dare dei problemi nella ricerca dei minimi tramite il gradiente.
\item[{Potential Regolation.}] Sia \(U:\R^{n}\to \R^{+}\) tale che:
\begin{enumerate}
\item \(U(x) = 0\) se e solo se \(x=0\);
\item \(U\) ha un minimo assoluto in \(x=0\).
\end{enumerate}

La funzione costo regolarizzata diventa:
\begin{equation*}
  G(\bm{w}) = C(\bm{w}) + \lambda\, U(\bm{w}),\qquad\lambda>0
\end{equation*}

Il potenziale deve essere scelto in maniera tale che l'errore, utilizzando \(G\), sia \uline{minore} che utilizzando \(C\).
\end{description}
\section{Processo di apprendimento di una rete neurale}
\label{sec:org320b895}
L'apprendimento di una \hyperref[sec:org18ff892]{rete neurale} è il processo di ricerca dei parametri ottimali per approssimare la funzione target. Questo è un processo iterativo algoritmico, che genera una \href{../../../../../org/roam/20250206170922-sequenze_e_stringhe.org}{sequenza} \((w_{t})\) di parametri. Poiché si parla di numeri immensi di elementi in questa sequenza, spesso ci si riferisce a \(t\) come una sorta di variabile temporale continua.

Si vuole allenare un modello per replicare una funzione target di cui si conoscono \(N\) valori: \(\set{(x_{i},z_{i})}\), minimizzando la funzione costo \(C(\bm{w})\).

Questo insieme è diviso in \uline{tre parti}:
\begin{itemize}
\item \uline{training set \(\mathcal{T}\)} (c.a. 70\% dei dati);
\item \uline{test set \(\mathrm{T}\)} (c.a. 20\% dei dati);
\item \uline{validation set \(\mathcal{V}\)} (c.a. 10\% dei dati).
\end{itemize}

Si suppone che siano identicamente distribuiti, e che siano indipendenti. Si ottengono quindi tre errori:
\begin{itemize}
\item \uline{errore di training \(C_{\mathcal{T}}(\bm{w})\)}: è il valore della funzione costo utilizzando i valori della funzione target presi dal training set;
\item \uline{errore di test \(C_{\mathrm{T}}(\bm{w})\)}: è il valore della funzione costo utilizzando i valori della funzione target presi dal test set;
\item \uline{validation error \(C_{\mathcal{V}}(\bm{w})\)}: è il valore della funzione costo utilizzando i valori della funzione target presi dal validation set.
\end{itemize}
\subsection{Errori di Training e di Test}
\label{sec:orgc41ea81}
Con un qualche algoritmo si trova il valore \(\bm{w}^{*}\) che minimizza \(C_{\mathcal{T}}\). Successivamente, si calcola \(C_{\mathrm{T}}(\bm{w}^{*})\), e generalmente vale:
\begin{equation*}
	C_{\mathcal{T}}(\bm{w}^{*}) \le C_{\mathrm{T}}(\bm{w}^{*})
\end{equation*}
Ci sono tre possibili scenari, a questo punto:
\begin{itemize}
\item sia \(C_{\mathcal{T}}(\bm{w}^{*})\) che \(C_{\mathrm{T}}(\bm{w}^{*})\) sono piccoli: questo è lo scenario desiderato;
\item \(C_{\mathcal{T}}(\bm{w}^{*})\) è piccolo, ma \(C_{\mathrm{T}}(\bm{w}^{*})\) è grande: questo è un fenomeno di \uline{overfitting}; questo significa che la rete neurale sta ``memorizzando'' il training set, e non riesce a generalizzare bene; probabilmente bisogna rivedere l'architettura della rete neurale, probabilmente diminuendo i parametri;
\item sia \(C_{\mathcal{T}}(\bm{w}^{*})\) che \(C_{\mathrm{T}}(\bm{w}^{*})\) sono grandi: questo è un fenomeno di \uline{underfitting}; bisogna rivedere l'architettura della rete neurale, probabilmente aumentando i parametri.
\end{itemize}

Pertanto si utilizza il \uline{test set} per verificare che i valori dei parametri trovati sul training set siano sufficientemente generalizzabili.
\subsection{Iperparametri di un processo di apprendimento}
\label{sec:orgd812bea}
L'algoritmo di apprendimento dipende da un insieme di parametri \uline{diversi} da quelli della rete neurale. Questi sono detti \uline{iperparametri}.

Si utilizza la mimimizzazione del \uline{validation error} proprio per regolare gli iperparametri.
\subsection{Alcuni esempi di algoritmi di apprendimento}
\label{sec:org65fcb17}

\subsubsection{Algoritmo di Regressione Lineare (Machine Learning)}
\label{sec:org72484c1}
\subsubsection{Algoritmo di Gradient Descent}
\label{sec:orgcb443aa}
\chapter{Cenni di Analisi Matematica}
\label{sec:orge5e4904}

\section{Teoria della misura}
\label{sec:orga3126cb}

\subsection{Funzioni sigmoidali e funzioni discriminatorie}
\label{sec:org0ac1b8c}

\begin{definizione}
Una funzione \(\sigma:\R\to [0,1]\) si dice \uline{sigmoidale} se\footnote{Vedi ``\href{../../../../../org/roam/20250625110412-limite_analisi_matematica.org}{Limite (Analisi Matematica)}''}
\begin{equation*}
\lim_{x\to-\infty} \sigma(x) = 0,\qquad \lim_{x\to +\infty} \sigma(x)= +1.
\end{equation*}
\end{definizione}
\begin{definizione}
Sia \(\mathcal{M}\) la famiglia delle \href{../../../../../org/roam/20250625104200-misura_di_baire.org}{misure di Baire} per \(\R^{n}\) sul cubo \(I^{n} \coloneqq [0,1]^{n} \subseteq \R\), \href{../../../../../org/roam/20250625110016-misura_finita.org}{finite}, \href{../../../../../org/roam/20250625110024-misura_con_segno.org}{con segno} e \href{../../../../../org/roam/20250625110032-misura_regolare.org}{regolari}.

Una funzione \(f: \R\to \R\) si dice \uline{discriminatoria per \(\mathcal{M}\)} se per ogni \(\mu \in \mathcal{M}\):
\begin{equation*}
\left(\forall \bm{w} \in \R^{n},\, \forall \theta \in \R\quad \int_{I^{n}} f(\bm{w}\cdot\bm{x}) \dif \mu(\bm{x}) = 0\right)\implies \mu=0
\end{equation*}
\end{definizione}
\begin{prop}
Ogni funzione \href{../../../../../org/roam/20250625110110-funzione_sigmoidale.org}{sigmoidale} \(\sigma:\R\to [0,1]\) è \href{../../../../../org/roam/20250625105528-funzione_discriminatoria_per_una_misura_di_baire_sul_cubo_unitario.org}{discriminatoria per \(\mathcal{M}\)}, dove \(\mathcal{M}\) è l'insieme \href{../../../../../org/roam/20250625104200-misura_di_baire.org}{misure di Baire} per \(\R^{n}\) sul cubo \(I^{n} \coloneqq [0,1]^{n} \subseteq \R\), \href{../../../../../org/roam/20250625110016-misura_finita.org}{finite}, \href{../../../../../org/roam/20250625110024-misura_con_segno.org}{con segno} e \href{../../../../../org/roam/20250625110032-misura_regolare.org}{regolare}.

Ovvero, se \(\sigma:\R\to [0,1]\) è tale che
\begin{equation*}
\lim_{x\to-\infty}\sigma(t) =0;\qquad \lim_{x\to+\infty}\sigma(t)=1
\end{equation*}
allora, per ogni \(\mu \in \mathcal{M}\),
\begin{equation*}
\left(\forall \bm{w} \in \R^{n},\, \forall \theta \in \R\quad \int_{I^{n}} f(\bm{w}\cdot\bm{x}) \dif \mu(\bm{x}) = 0\right)\implies \mu=0
\end{equation*}
\end{prop}
\section{Minimizzazione}
\label{sec:org63a8260}

\begin{definizione}
\lipsum[1]
\end{definizione}
\begin{prop}
Sia \(A \subseteq \R^{n}\) aperto e sia \(f \in C^{2}(A)\)\footnote{Vedi ``\href{../../../../../org/roam/20250113125602-classe_c_di_una_funzione.org}{Funzione di classe Ck}''}. Sia \(c\) un \href{../../../../../org/roam/20250702101346-punto_critico_di_una_funzione_reale.org}{punto critico} per \(f\). Sia \(H_{f}(c)\) l'\href{../../../../../org/roam/20250627161731-matrice_hessiana.org}{Hessiana} di \(f\) calcolata nel punto \(c\).
\begin{itemize}
\item Se \(H_{f}(c)\) è \href{../../../../../org/roam/20250702102213-matrice_definita_positiva.org}{definita positiva}, allora \(c\) è un punto di \href{../../../../../org/roam/20250627153543-massimo_e_minimo_di_una_funzione_reale.org}{minimo locale forte}.
\item Se \(H_{f}(c)\) è \href{../../../../../org/roam/20250702102213-matrice_definita_positiva.org}{definita negativa}, allora \(c\) è un punto di \href{../../../../../org/roam/20250627153543-massimo_e_minimo_di_una_funzione_reale.org}{massimo locale forte}.
\item Se \(H_{f}(c)\) è \href{../../../../../org/roam/20250702102213-matrice_definita_positiva.org}{indefinita}, allora \(c\) è un punto di \href{../../../../../org/roam/20250702102107-punto_di_sella.org}{sella}.
\end{itemize}
\end{prop}

\begin{proof}
Si dimostra che se \(H_{f}(c)\) è definita positiva, allora \(c\) è un punto di minimo locale.

Sia \(\gamma:[-\varepsilon,\varepsilon]\to A\) una curva di classe \(C^{2}\) tale che \(\gamma(0) = c\) e \(\norma{\gamma'(0)} = 1\). Sia \(\bm{v} \coloneqq \gamma'(0)\). Sia \(g(t) \coloneqq f\circ\gamma(t)\).
\begin{itemize}
\item \(g'(0) = 0\). Infatti, applicando la \href{../../../../../org/roam/20250702114407-chain_rule.org}{chain rule}:\footnote{Vedi ``\href{../../../../../org/roam/20250624171244-gradiente_di_una_funzione.org}{Gradiente di una funzione}''}
\begin{align*}
  g'(0) &= \nabla f (\gamma(0)) \cdot \gamma'(0)\\
  &= \nabla f(c) \cdot \bm{v} = 0\cdot \bm{v}=0.
\end{align*}
\item \(g''(0) >0\). Infatti, si noti che\footnote{Vedi ``\href{../../../../../org/roam/20250702114642-derivata_direzionale.org}{Derivata direzionale}''}
\begin{equation*}
  g''(0) = D_{\bm{v}}\left(D_{\bm{v}} f\right)(c)
\end{equation*}
Inoltre, siccome \(f\) è differenziabile, si ha che \(D_{\bm{v}} f(x) = \nabla f (x)\cdot \bm{v}\).

Anche \(D_{\bm{v}}f\) è differenziabile, e pertanto <
\begin{equation*}
  D_{\bm{v}}\left(D_{\bm{v}} f\right)(x) = \nabla (D_{\bm{w}}f) (x)\cdot \bm{v}
\end{equation*}
Ma
\begin{equation*}
  \nabla(\nabla f (x)\cdot \bm{v}) = H_{f}(x) \bm{v}
\end{equation*}
e pertanto \(g''(0) = H_{f}(c)\bm{v}\cdot\bm{v}>0\).\qedhere
\end{itemize}
\end{proof}
\begin{prop}
\lipsum[1]
\end{prop}
\begin{lem}
\lipsum[2]
\end{lem}

\begin{lem}
\lipsum[3]
\end{lem}

\begin{thm}
\lipsum[4]
\end{thm}
\part{Cordero}
\chapter{Teoremi di approssimazione}
\label{sec:org74585fb}

\section{Teoremi di Dini per la convergenza uniforme}
\label{sec:org0955ec5}
\begin{thm}
Sia \(f_{n}:[a,b]\to \R\) una \href{../../../../../org/roam/20250629105815-successione_di_funzioni.org}{successione di funzioni} \href{../../../../../org/roam/20250103103252-funzione_continua.org}{continue}.
\begin{enumerate}
\item Se per ogni \(n \in \N\): \(f_{n+1}\le f_{n}\) e
\begin{equation*}
 \forall x \in [a,b]\ f_{n}(x)\to 0
\end{equation*}
allora \(f_{n}\) \href{../../../../../org/roam/20250629105745-convergenza_uniforme.org}{converge uniformemente} a \(0\) su \([a,b]\).
\item Sia \(g:[a,b]\to \R\) continua. Se per ogni \(n \in \N\): \(f_{n+1}\le f_{n}\) e
\begin{equation*}
 \forall x \in [a,b]\ f_{n}(x)\to g(x)
\end{equation*}
allora \(f_{n}\) \href{../../../../../org/roam/20250629105745-convergenza_uniforme.org}{converge uniformemente} a \(g\) su \([a,b]\).
\end{enumerate}
\end{thm}
\section{Teorema di Ascoli-Arzelà}
\label{sec:orgec775db}

\begin{definizione}
Una famiglia di funzioni \(\mathcal{F}\) da un insieme \(A\) a valori reali si dice \uline{uniformemente limitata} (\emph{uniformly bounded}) se esiste \(M > 0\) tale che
\begin{equation*}
\forall x \in A,\ \forall f \in \mathcal{F}\ |f(x)|\le M.
\end{equation*}
\end{definizione}

\begin{esempio}
Sia \(\mathcal{F} \coloneqq \set{\cos(ax+b); a,b \in \R}\). Allora la famiglia \(\mathcal{F}\) è uniformemente limitata su \(\R\), per \(M=1\).
\end{esempio}

\begin{esempio}
Si consideri
\begin{equation*}
\mathcal{F}\coloneqq \set{\sum_{j=1}^{N} \alpha_{j}\sigma(w_{j}x+b_{j}); \alpha_{j}, w_{j}, b_{j} \in \R\mid \sum_{j=1}^{N}\alpha_{j}^{2} \le 1}
\end{equation*}
dove \(\sigma(x)\) è una \href{../../../../../org/roam/20250625110110-funzione_sigmoidale.org}{funzione sigmoidale} fissata.

Questa famiglia è uniformemente limitata su \(\R\), per \(M=\sqrt{N}\), poiché, per \href{../../../../../org/roam/20250629112810-disuguaglianza_di_cauchy_schwarz.org}{C-S}
\begin{equation*}
\bigg\lvert\,\sum_{j=1}^{N}\alpha_{j}\sigma(w_{j}x+b_{j})\,\bigg\rvert^{2}\le \left(\sum_{j=1}^{N}\alpha_{j}\right)\cdot\left(\sum_{j=1}^{N}\sigma(w_{j}x+b_{j})\right)\le 1\cdot N=N.
\end{equation*}
\end{esempio}
\begin{definizione}
Una famiglia di funzioni \(\mathcal{F}\) da un insieme \(A \subseteq \R\) a valori reali si dice \uline{equicontinua} se per ogni \(\varepsilon>0\) esiste \(\delta>0\) tale che
\begin{equation*}
\forall f \in \mathcal{F}\ \forall x,y \in A\ \left(|x-y|<\delta\implies |f(x)-f(y)|<\varepsilon\right).
\end{equation*}
Equivalentemente, le funzioni di \(\mathcal{F}\) sono \href{../../../../../org/roam/20250611135127-funzione_uniformemente_continua.org}{uniformemente continua} per gli stessi \(\varepsilon\) e \(\delta\).
\end{definizione}

\begin{esempio}
Si consideri \(\mathcal{F} \subseteq C^{1}\left([a,b]\right)\)\footnote{Vedi ``\href{../../../../../org/roam/20250113125602-classe_c_di_una_funzione.org}{Classe C di una funzione}''} tale che esista \(L>0\):
\begin{equation*}
\forall f \in \mathcal{F}\ \sup_{x \in [a,b]}|f'(x)|<L.
\end{equation*}

Allora \(\mathcal{F}\) è equicontinua.

Infatti, per il \href{../../../../../org/roam/20250629143200-teorema_di_lagrange.org}{Teorema di Lagrange}:
\begin{equation*}
|f(x)-f(y)|\le \left(\sup_{c \in [a,b]} |f'(c)|\right)\ |x-y| \le L\ |x-y|
\end{equation*}
e dunque, scegliendo \(\delta = \varepsilon/L\) si ha la tesi.
\end{esempio}

\begin{esempio}
Sia \(\sigma\) una funzione sigmoidale tale che esista \(\lambda>0\):
\begin{equation*}
\forall x \in \R:\ |\sigma'(x)|<\lambda<1
\end{equation*}
e sia \(\mathcal{F}\) la famiglia definita su \([a,b] \subseteq \R\) come segue:
\begin{equation*}
\mathcal{F} \coloneqq \set{\sum_{j=1}^{N}\alpha_{j} \sigma(w_{j}x+b_{j}); \alpha_{j},w_{j},b_{j} \in \R\mid \sum_{j=1}^{N}(\alpha_{j}^{2}+w_{j}^{2})\le 1}.
\end{equation*}

Allora \(\mathcal{F}\) è equicontinua. Infatti:
\begin{itemize}
\item \(\mathcal{F} \subseteq C^{1}\left([a,b]\right)\);
\item Sia \(f \in \mathcal{F}\): per \href{../../../../../org/roam/20250629112810-disuguaglianza_di_cauchy_schwarz.org}{C-S}:
\begin{align*}
  |f'(x) | &= \bigg\lvert \sum_{j=1}^{N} \alpha_{j}w_{j} \sigma'(w_{j}x+b_{j})\bigg\rvert \le \sum_{j=1}^{N} |\alpha_{j}|\, |w_{j}|\,\lambda\\
  &\le \lambda \left(\sum_{j=1}^{N} \alpha_{j}^{2}\right)^{1/2}\cdot\left(\sum_{j=1}^{N}w_{j}^{2}\right)^{1/2}\le\lambda.
\end{align*}
\end{itemize}
Per l'esempio precedente, si ottieene la tesi.

Si noti, inoltre, che se \(\sigma\) è la \hyperref[sec:orgeed12bd]{sigmoide} logistica, allora la derivata massima è ottenuto in \(x=0\) e vale
\begin{equation*}
\sigma'(0) = \sigma(0)\,(1-\sigma(0)) = \frac{1}{4}<1.
\end{equation*}
\end{esempio}
\begin{thm}
Sia \(\mathcal{F}\) una famiglia di funzioni continue su \([a,b]\) a valori reali. Sono fatti equivalenti:
\begin{enumerate}
\item Ogni \href{../../../../../org/roam/20250629105815-successione_di_funzioni.org}{successione} \((f_{n})_{n \in \N} \subseteq \mathcal{F}\) contiene una \href{../../../../../org/roam/20250115100916-sottosuccessione.org}{sottosuccessione} \((f_{n_{k}})_{k \in \N}\) che \href{../../../../../org/roam/20250629105745-convergenza_uniforme.org}{converge uniformemente}.
\item La famiglia \(\mathcal{F}\) è \href{../../../../../org/roam/20250629113211-famiglia_di_funzioni_equicontinua.org}{equicontinua} e \href{../../../../../org/roam/20250629110306-funzioni_uniformemente_limitate.org}{uniformemente limitata}.
\end{enumerate}
\end{thm}

Ecco alcuni semplici corollari del Teorema di Ascoli-Arzelà.

\begin{thm}
Sia \(N\ge {1}\) un intero fissato, e si consideri una \hyperref[sec:org18ff892]{rete neurale} con un \hyperref[sec:orgf3e3b34]{layer nascosto} tale che:
\begin{enumerate}
\item l'input della rete sia una variabile reale \(x \in [a,b]\);
\item l'output della rete sia un \hyperref[sec:org3008588]{neurone} unidimensionale con \hyperref[sec:orgeed12bd]{funzione di attivazione lineare} e zero bias;
\item ci sono \(N\) neuroni nel layer nascosti con una funzione di attivazione differenziabile tale che \(|\sigma'|<\lambda<1\);
\item i pesi soddisfano la condizione di \hyperref[sec:org2dcb0b4]{regolarizzazione}:
\begin{equation*}
 \sum_{j=1}^{N} (\alpha_{j}^{2} + w_{j}^{2})\le 1
\end{equation*}
dove i \(w_{j}\) sono i pesi per l'input al layer nascosto, e gli \(a_{j}\) sono i pesi dal layer nascosto all'output.
\end{enumerate}

Allora esiste una \href{../../../../../org/roam/20250103103252-funzione_continua.org}{funzione continua} \(g:[a,b]\to \R\) che può essere approssimata dalla rete.
\end{thm}

\begin{proof}
È sufficiente mostrare che la famiglia di funzioni output del sistema sia composta da funzioni continue, e che sia \href{../../../../../org/roam/20250629113211-famiglia_di_funzioni_equicontinua.org}{equicontinua} e \href{../../../../../org/roam/20250629110306-funzioni_uniformemente_limitate.org}{uniformemente limitata}.

Con una banale applicazione del \href{../../../../../org/roam/20250629120441-teorema_di_ascoli_arzela.org}{Teorema di Ascoli-Arzelà}, si ottiene che ogni \href{../../../../../org/roam/20250629105815-successione_di_funzioni.org}{successione} di funzioni output ammette una sottosuccessione \href{../../../../../org/roam/20250629105745-convergenza_uniforme.org}{convergente uniformemente} ad una funzione continua; quest'ultima, data la uniforme convergenza, viene approssimata dalla rete con un grado di accuratezza arbitrario.
\end{proof}

\begin{prop}
Anche per una \hyperref[sec:org18ff892]{rete neurale} composta da un unico \hyperref[sec:org3008588]{neurone}, con output
\begin{equation*}
f_{\bm{w},b}(\bm{x}) = \sigma(\bm{w}\cdot\bm{x} + b)
\end{equation*}
dove \(\sigma\) è la \hyperref[sec:org1d40102]{funzione logistica}, con pesi \(\bm{w} \in \R^{n}, b \in \R\) e input \(\bm{x} \in I_{n} \coloneqq [0,1]^{n}\), esiste una \href{../../../../../org/roam/20250103103252-funzione_continua.org}{funzione continua} \(g:I^{n}\to \R\) che può essere approssimata. Si supponga che \(\norma{\bm{w}}\le 1\).
\end{prop}


\begin{proof}
Infatti, la famiglia di funzioni continue
\begin{equation*}
\mathcal{F} \coloneqq \set{f_{\bm{w},b}\mid \norma{\bm{w}}\le 1, b \in \R}
\end{equation*}
è \href{../../../../../org/roam/20250629110306-funzioni_uniformemente_limitate.org}{uniformemente limitata} poiché \(|f_{\bm{w},b}|< 1\), ed inoltre è equicontinua, in quanto, per il \href{../../../../../org/roam/20250629143200-teorema_di_lagrange.org}{Teorema di Lagrange}
\begin{align*}
|f_{\bm{w},b}(\bm{x})-f_{\bm{w},b}(\bm{y}) | &= \big\lvert \sigma(\bm{w}\cdot\bm{x} + b) - \sigma(\bm{w}\cdot\bm{y} + b)\big\rvert\\
&\le \max|\sigma'|\ |\bm{w}\cdot\bm{x} + b - \bm{w}\cdot\bm{y} - b|\\
&= \frac{1}{4} |\bm{w}\cdot(\bm{x}-\bm{y})| \le \frac{1}{4} \norma{\bm{w}}\cdot \norma{\bm{x}-\bm{y}} \\
&\le \frac{1}{4}\norma{\bm{x}-\bm{y}}.
\end{align*}

Con una banale applicazione del \href{../../../../../org/roam/20250629120441-teorema_di_ascoli_arzela.org}{Teorema di Ascoli-Arzelà}, si ottiene che ogni \href{../../../../../org/roam/20250629105815-successione_di_funzioni.org}{successione} di funzioni output ammette una sottosuccessione \href{../../../../../org/roam/20250629105745-convergenza_uniforme.org}{convergente uniformemente} ad una funzione continua; quest'ultima, data la uniforme convergenza, viene approssimata dalla rete con un grado di accuratezza arbitrario.
\end{proof}
\section{Teorema di Stone Weierstrass}
\label{sec:orgd20c978}

\begin{definizione}
Sia \(\mathcal{F}\) un insieme di \href{../../../../../org/roam/20250202170607-classe_relazione_binaria.org}{funzioni} con lo stesso \href{../../../../../org/roam/20250202173528-dominio_range_e_campo_di_una_classe_relazione.org}{dominio}, a \href{../../../../../org/roam/20250202173528-dominio_range_e_campo_di_una_classe_relazione.org}{valori} in \(\R\). \(\mathcal{F}\) si dice un'\uline{algebra di funzioni reali} se, per ogni \(f,g \in \mathcal{F}\) e \(c \in \R\):
\begin{enumerate}
\item \(f+g \in \mathcal{F}\);
\item \(c\,f \in \mathcal{F}\);
\item \(f\,g \in \mathcal{F}\).
\end{enumerate}
\end{definizione}

Questa è la definizione di \href{../../../../../org/roam/20250110175552-algebra_su_un_campo.org}{\(\R\)-algebra}, specializzata in questo ambito.

\begin{esempio}
Sia \(\mathcal{A}\) l'insieme di tutte le serie di Fourier finite su \([0,2\pi]\):
\begin{equation*}
\mathcal{A} \coloneqq \set{f(x) = c_{0}+ \sum_{k=1}^{N} (a_{k}\cos kx + b_{k}\sin kx)\mid c_{0},a_{k},b_{k} \in \R, N \in\N}.
\end{equation*}
Ovviamente \(\mathcal{A}\) è chiuso per combinazioni lineari. Utilizzando il fatto che
\begin{equation*}
\cos(mx)\cos(nx) = \frac{1}{2}\left[\cos((m+n)x) + \cos((m-n)x)\right]
\end{equation*}
segue che \(\mathcal{A}\) è anche chiuso per prodotti. Pertanto è una \(\R\)-algebra.
\end{esempio}
\begin{definizione}
Sia \(\mathcal{A}\) una \(\R\)-\href{../../../../../org/roam/20250110175552-algebra_su_un_campo.org}{algebra} di funzioni di dominio \(K \subseteq \R^{n}\) a valori in \(\R\).

Si dice che \(\mathcal{A}\) \uline{separa i punti} se per ogni \(x,y \in K\) esistono \(f,g \in \mathcal{A}\) tali che
\begin{equation*}
f(x) \neq f(y).
\end{equation*}
\end{definizione}

\begin{esempio}
Sia \(\mathcal{A}\) l'insieme dei polinomi definiti su \([a,b]\):
\begin{equation*}
\mathcal{A} \coloneqq \set{\restriction{f}{[a,b]}\mid f(x) = \sum_{k=0}^{n} c_{k}x^{k}\mid c_{k} \in \R, n \in \N}.
\end{equation*}
Ovviamente \(\mathcal{A}\) è una \(\R\)-\href{../../../../../org/roam/20250629165520-algebra_di_funzioni_reali.org}{algebra di funzioni}, e inoltre separa i punti, poiché \(f(x) = \Id_{[a,b]} \in \mathcal{A}\)\footnote{Vedi ``\href{../../../../../org/roam/20250310111151-funzione_identita.org}{Funzione identità}''}.

Inoltre \(1 \in \mathcal{A}\), e pertanto \(\mathcal{A}\) contiene tutte le funzioni costanti.
\end{esempio}
\begin{thm}
Sia \(K \subseteq \R^{n}\) \href{../../../../../org/roam/20250103163701-spazio_topologico_compatto.org}{compatto}, e sia \(\mathcal{A} \subseteq C(K)\)\footnote{Vedi ``\href{../../../../../org/roam/20250113125602-classe_c_di_una_funzione.org}{Classe C di una funzione}''\label{orgb2e8941}} una \href{../../../../../org/roam/20250629165520-algebra_di_funzioni_reali.org}{\(\R\)-algebra}. Se
\begin{enumerate}
\item \(\mathcal{A}\) separa i punti di \(K\);
\item \(\mathcal{A}\) contiene le funzioni costanti;
\end{enumerate}
allora \(\mathcal{A}\) è un \href{../../../../../org/roam/20250301193045-sottoinsieme_denso.org}{sottoinsieme denso} di \(C(K)\) munito della \href{../../../../../org/roam/20250103145124-topologia.org}{topologia} \href{../../../../../org/roam/20250301193530-topologia_indotta_da_una_distanza.org}{indotta dalla metrica}\footnote{Tale massimo esiste per il \href{../../../../../org/roam/20250627153319-teorema_di_weierstrass.org}{Teorema di Weierstrass}\label{orgb234935}}:
\begin{equation*}
\forall f,g \in C(K):\quad d(f,g) \coloneqq \max_{x \in K} |f(x)-g(x)|.
\end{equation*}
\end{thm}
\subsection{Corollari del teorema}
\label{sec:orgbb843b0}

\begin{prop}
Sia \([a,b] \subseteq \R\), e si consideri \(C([a,b])\)\textsuperscript{\ref{orgb2e8941}} munito della \href{../../../../../org/roam/20250103145124-topologia.org}{topologia} \href{../../../../../org/roam/20250301193530-topologia_indotta_da_una_distanza.org}{indotta dalla metrica}\textsuperscript{\ref{orgb234935}}:
\begin{equation*}
\forall f,g \in C(K):\quad d(f,g) \coloneqq \max_{x \in [a,b]} |f(x)-g(x)|.
\end{equation*}
Sia:
\begin{equation*}
\mathcal{A} \coloneqq \set{\restriction{f}{[a,b]}\mid f(x) = \sum_{k=0}^{n} a_{k}x^{k}; a_{k} \in \R, n \in \N}.
\end{equation*}

Allora \(\mathcal{A}\) è \href{../../../../../org/roam/20250301193045-sottoinsieme_denso.org}{denso} in \(C([a,b])\), ovvero: per ogni \(f:[a,b]\to \R\) \href{../../../../../org/roam/20250103103252-funzione_continua.org}{continua} e per ogni \(\varepsilon>0\) esiste \(g \in \mathcal{A}\) tale che
\begin{equation*}
\max_{x \in [a,b]} |f(x)-g(x)|<\varepsilon
\end{equation*}
\end{prop}

\begin{proof}
\(\mathcal{A}\) è una \href{../../../../../org/roam/20250629165520-algebra_di_funzioni_reali.org}{\(\R\)-algebra} che \href{../../../../../org/roam/20250629151420-algebra_di_funzioni_separa_i_punti.org}{separa i punti} e contiene le funzioni costanti.
\end{proof}

\begin{prop}
Sia \(f:[a,b]\times[c,d]\to \R\) una funzione continua. Allora, per ogni \(\varepsilon>0\) esiste \(N\ge 1\) ed esistono, per ogni \(i=1,\dots,N\), delle \(g_{i} \in C([a,b])\)\textsuperscript{\ref{orgb2e8941}} e \(h_{i} \in C([c,d])\) tali che
\begin{equation*}
\max_{\substack{
x \in [a,b]\\
y \in [c,d]}}
\bigg\lvert
f(x,y)-\sum_{i=1}^{N}g_{i}(x)h_{i}(y)
\bigg\rvert<\varepsilon
\end{equation*}
\end{prop}

\begin{proof}
Si consideri
\begin{equation*}
\mathcal{A}\coloneqq \set{
G(x,y) = \sum_{i=1}^{N} g_{i}(x)h_{i}(y)\mid g_{i} \in C([a,b]), h_{i} \in C([c,d]), N = 1,2,\dots
}
\end{equation*}
Si osserva facilmente che \(\mathcal{A}\) è chiuso per somme, prodotti e prodotti per scalari, ovvero è una \href{../../../../../org/roam/20250629165520-algebra_di_funzioni_reali.org}{\(\R\)-algebra}. Inoltre contiene le funzioni costanti.

Se \((x_{0},y_{0})\neq (x_{1},y_{1})\):
\begin{itemize}
\item se \(x_{0}\neq x_{1}\) allora \(G(x,y) = x\cdot 1 \in \mathcal{A}\) \href{../../../../../org/roam/20250629151420-algebra_di_funzioni_separa_i_punti.org}{separa i due punti};
\item se \((y_{0}\neq y_{1})\) allora \(G(x,y) = 1\cdot y \in \mathcal{A}\) \href{../../../../../org/roam/20250629151420-algebra_di_funzioni_separa_i_punti.org}{separa i due punti}.\qedhere
\end{itemize}
\end{proof}
\subsection{Applicazioni alle reti neurali}
\label{sec:orgc15527e}

\begin{prop}
Per ogni \href{../../../../../org/roam/20250630103918-funzione_periodica.org}{funzione periodica} \(F:\R\to \R\) esiste una \hyperref[sec:org18ff892]{rete neurale} che la approssima.
\end{prop}

\begin{proof}
Sia \(T\) il periodo di \(F\), ovvero \(T \in \R\) tale che
\begin{equation*}
\forall t \in \R\quad F(t+T) = F(t).
\end{equation*}
Si consideri
\begin{equation*}
\mathcal{A} \coloneqq \set{
\restriction{f}{[0,T]}\mid f(x) = a_{0}+\sum_{j=1}^{n} a_{j}\cos\left(\frac{2\pi}{T}\,jx\right) + c_{j}\sin \left(\frac{2\pi}{T}\, jx\right); a_{i}, c_{i} \in \R, n=1,2,\dots
}
\end{equation*}
Si noti che per ogni \(f \in \mathcal{A}\)
\begin{equation*}
f(0) = f(T).
\end{equation*}

L'insieme \(\mathcal{A} \subseteq C([0,T])\)\footnote{Vedi ``\href{../../../../../org/roam/20250113125602-classe_c_di_una_funzione.org}{Classe C di una funzione}''\label{orgc8f3d78}}, ed inoltre è una \href{../../../../../org/roam/20250629165520-algebra_di_funzioni_reali.org}{\(\R\)-algebra}\footnote{Questo si vede facilmente seguendo l'Esempio di ``\href{../../../../../org/roam/20250629165520-algebra_di_funzioni_reali.org}{Algebra di funzioni reali}''}. Contiene tutte le funzioni costanti (basta porre \(a_{j}=c_{j} = 0\) per ogni \(j>0\)), e separa i punti, in quanto
\begin{equation*}
g(x) = \cos\left(\frac{\pi}{T}\,x\right) \in \mathcal{A}
\end{equation*}
è una biiezione tra \([0,T]\) e \([0,1]\).

Dunque per il \href{../../../../../org/roam/20250629165421-teorema_di_stone_weierstrass.org}{Teorema di Stone-Weierstrass} \(\mathcal{A}\) è \href{../../../../../org/roam/20250301193045-sottoinsieme_denso.org}{denso} in \(C([0,T])\) ed in particolare, siccome \(F \in C([0,T])\), per ogni \(\varepsilon>0\), esiste \(N \in \N\) tale che
\begin{gather*}
G(x) \coloneqq a_{0}+\sum_{j=1}^{N} a_{j}\cos\left(\frac{2\pi}{T}\,jx\right) + c_{j}\sin \left(\frac{2\pi}{T}\, jx\right) \in \mathcal{A}\\
\max_{x \in [0,T]} \left|G(x)- F(x)\right| = \max_{x \in \R} \left|G(x)-F(x)\right| <\varepsilon
\end{gather*}

Si consideri una rete neurale fatta come segue:
\begin{itemize}
\item input: \(x \in \R\);
\item un \hyperref[sec:orgf3e3b34]{layer nascosto} con \(N\) neuroni con funzione di attivazione \(\cos\), con peso dall'input \(w_{j}\) e bias \(b_{j}\);
\item il neurone di output con funzione di attivazione lineare, bias \(a_{0}\) e pesi dall'hidden layer \(\alpha_{j}\).
\end{itemize}
Questa rete è rappresentata in Fig. \ref{fig:retperiod} e produce output
\begin{equation*}
y = a_{0} + \sum_{j=1}^{N}\alpha_{j}\cos(w_{j}x+b_{j}).
\end{equation*}
Si dimostra che tale rete neurale è in grado di produrre come output \(G(x)\), ovvero che, fissato un livello di precisione \(\varepsilon\), esiste una rete neurale che produce come output una funzione periodica che approssima \(F\) con quel livello di precisione.

Infatti, ponendo
\begin{equation*}
w_{j} \coloneqq \frac{2\pi}{T}
\end{equation*}
e \(\alpha_{j}, b_{j}\) tali che
\begin{equation*}
\begin{cases}
a_{j} = \alpha_{j}\cos b_{j}\\
c_{j} = -\alpha_{j}\sin b_{j}
\end{cases}
\end{equation*}
si ottiene che, detto per semplicità di notazione \(\nu\coloneqq 2\pi/T\)
\begin{align*}
y &=  a_{0} + \sum_{j=1}^{N}\alpha_{j}\cos(w_{j}x+b_{j})\\
&= a_{0} + \sum_{j=1}^{N} \alpha_{j}\cos (\nu j x) \cos b_{j} - \alpha_{j}\sin(\nu j x) \sin b_{j}\\
&= a_{0} + \sum_{j=1}^{N} a_{j} \cos(\nu j x) + c_{j}\sin(\nu j x)\\
&= a_{0} + \sum_{j=1}^{N} a_{j} \cos\left(\frac{2\pi}{T}\,jx\right) + c_{j}\sin \left(\frac{2\pi}{T}\,jx\right) = G(x).\qedhere
\end{align*}
\end{proof}

\begin{figure}
\begin{equation*}
\begin{tikzcd}[ampersand replacement=\&,cramped]
	\& 1 \\
	\&\& {\boxed{\cos}} \\
	\&\& {\boxed{\cos}} \\
	{\textcolor{red}{x}} \&\& {\boxed{\cos}} \&\&\& {\boxed{\text{Funzione lineare}}} \& {\textcolor{red}{y}} \\
	\&\& \vdots \\
	\&\& {\boxed{\cos}} \&\& 1 \\
	\& 1
	\arrow["{b_1}"{description}, from=1-2, to=2-3]
	\arrow["{\alpha_1}"{description}, from=2-3, to=4-6]
	\arrow["{\alpha_2}"{description}, from=3-3, to=4-6]
	\arrow["{w_1}"{description}, from=4-1, to=2-3]
	\arrow["{w_2}"{description}, from=4-1, to=3-3]
	\arrow["{w_3}"{description}, from=4-1, to=4-3]
	\arrow["{w_j}"{description}, from=4-1, to=5-3]
	\arrow["{w_N}"{description}, from=4-1, to=6-3]
	\arrow["{\alpha_3}"{description}, from=4-3, to=4-6]
	\arrow[from=4-6, to=4-7]
	\arrow["{\alpha_j}"{description}, from=5-3, to=4-6]
	\arrow["{\alpha_N}"{description}, from=6-3, to=4-6]
	\arrow["{a_0}"{description}, from=6-5, to=4-6]
	\arrow["{b_N}"{description}, from=7-2, to=6-3]
\end{tikzcd}
\end{equation*}
\caption{La rete neurale considerata}\label{fig:retperiod}
\end{figure}
\section{Teoremi Tauberiani di Wiener}
\label{sec:org8729b85}

Si consideri l'operatore funzionale
\begin{equation*}
T_{\theta}: f(x)\mapsto f(x-\theta).
\end{equation*}

\begin{thm}
Sia \(f \in L^{1}(\R)\)\footnote{Vedi ``\href{../../../../../org/roam/20250624162220-spazi_lp.org}{Spazi Lp}''\label{org91cf1c7}}. Lo \href{../../../../../org/roam/20250630122400-span.org}{span} di \(\set{T_{\theta}f\mid \theta \in \R}\) è \href{../../../../../org/roam/20250301193045-sottoinsieme_denso.org}{denso} in \(L^{1}(\R)\) se e solo se la \href{../../../../../org/roam/20250630121906-trasformata_di_fourier.org}{trasformata di Fourier}:
\begin{equation*}
\forall \xi \in \R:\qquad \hat{f}(\xi) \neq 0
\end{equation*}
\label{teo:wt1}
\end{thm}

\begin{thm}
Sia \(f \in L^{2}(\R)\)\textsuperscript{\ref{org91cf1c7}}. Lo \href{../../../../../org/roam/20250630122400-span.org}{span} di \(\set{T_{\theta}f\mid \theta \in \R}\) è \href{../../../../../org/roam/20250301193045-sottoinsieme_denso.org}{denso} in \(L^{2}(\R)\) se e solo se gli zeri della \href{../../../../../org/roam/20250630121906-trasformata_di_fourier.org}{trasformata di Fourier}\footnote{Vedi ``\href{../../../../../org/roam/20250630122745-proprieta_vera_quasi_ovunque.org}{Proprietà vera quasi ovunque}''}
\begin{equation*}
\hat{f}(\xi) \neq 0\text{ q.o. }\xi \in \R
\end{equation*}
ovvero per la \href{../../../../../org/roam/20250630122824-misura_di_lebesgue.org}{misura di Lebsegue} \(\mu\):
\begin{equation*}
\mu\left(\set{\xi \in \R\mid \hat{f}(\xi) = 0}\right) = 0
\end{equation*}
\label{teo:wt2}
\end{thm}
\subsection{Applicazioni dei Teoremi Tauberiani di Wiener al Machine Learning}
\label{sec:org89347da}
\begin{enumerate}
\item Sia \(g \in L^{1}(\R)\), e sia \(f \in L^{1}(\R)\) tale che
\begin{equation*}
  \forall \xi \in \R:\qquad \hat{f}(\xi) \neq 0
\end{equation*}
(ovvero \(f\) soddisfa le ipotesi del Teorema \ref{teo:wt1})

Allora, per ogni \(\varepsilon>0\) esiste \(N \in \N\) ed esistono, per \(j=1,\dots,N\), degli \(\alpha_{j},\theta_{j} \in \R\) tali che
\begin{gather*}
  G(x) \coloneqq \sum_{j=1}^{N} \alpha_{j} f(x+\theta_{j})\\
  \int_{\R}|g(x)-G(x)|\dif x<\varepsilon.
\end{gather*}

La funzione \(G(x)\) è l'output di una \hyperref[sec:org18ff892]{rete neurale} con un \hyperref[sec:orgf3e3b34]{layer nascosto} di \(N\) neuroni (e neurone di output lineare), dove i \hyperref[sec:org3008588]{neuroni} del layer nascosto hanno funzione di attivazione \(f\).
\item Sia \(g \in L^{2}(\R)\), e sia \(f \in L^{2}(\R)\) tale che
\begin{equation*}
  \mu\left(\set{\xi \in \R\mid f(\xi)=0}\right)=0
\end{equation*}
(ovvero \(f\) soddisfa le ipotesi del Teorema \ref{teo:wt2})

Allora, per ogni \(\varepsilon>0\) esiste \(N \in \N\) ed esistono, per \(j=1,\dots,N\), degli \(\alpha_{j},\theta_{j} \in \R\) tali che
\begin{gather*}
  G(x) \coloneqq \sum_{j=1}^{N} \alpha_{j} f(x+\theta_{j})\\
  \int_{\R}(g(x)-G(x))^{2}\dif x<\varepsilon.
\end{gather*}

La funzione \(G(x)\) è l'output di una \hyperref[sec:org18ff892]{rete neurale} con un \hyperref[sec:orgf3e3b34]{layer nascosto} di \(N\) neuroni (e neurone di output lineare), dove i \hyperref[sec:org3008588]{neuroni} del layer nascosto hanno funzione di attivazione \(f\).
\end{enumerate}

Si noti che le seguenti \hyperref[sec:orgeed12bd]{funzioni di attivazione} soddisfano le ipotesi dei Teoremi \ref{teo:wt1} e \ref{teo:wt2}.
\begin{description}
\item[{Doppio esponenziale.}] Sia \(f(x) = e^{-\lambda|x|}\), con \(\lambda>0\).
\begin{equation*}
  \int_{\R}|f(x)|\dif x = 2\int_{0}^{+\infty} e^{-\lambda x}\dif x = -\frac{2}{\lambda} [e^{-\lambda x}]^{ +\infty}_{0} = \frac{2}{\lambda}<\infty
\end{equation*}
e dunque \(f \in L^{1}(\R)\). Inoltre
\begin{equation*}
  \int_{\R} (f(x))^{2}\dif x = \int_{\R} e^{-2\lambda|x|}\dif x = \frac{1}{\lambda}<\infty
\end{equation*}
e pertanto \(f \in L^{2}(\R)\).

Si calcola la trasformata di Laplace:
\begin{align*}
  \hat{f}(\xi) &= \int_{\R} e^{-2\pi i \xi x} e^{-\lambda|x|} \dif x\\
  &= \int_{-\infty}^{0 } e^{-2\pi i \xi x} e^{-\lambda|x|} \dif x + \int_{0}^{+ \infty} e^{-2\pi i \xi x} e^{-\lambda|x|} \dif x\\
  &= \int_{-\infty}^{0} e^{-2\pi i \xi x} e^{\lambda x} \dif x + \int_{0}^{+ \infty} e^{-2\pi i \xi x} e^{-\lambda x} \dif x\\
  &= \int_{0}^{+ \infty} e^{2\pi i \xi x} e^{-\lambda x} \dif x + \int_{0}^{+ \infty} e^{-2\pi i \xi x} e^{-\lambda x} \dif x\\
  &= \int_{0}^{+ \infty} e^{- (-2\pi i \xi +\lambda)x}\dif x + \int_{0}^{ + \infty} e^{- (2\pi i \xi +\lambda) x}\dif x\\
  &= \frac{1}{-2\pi i \xi +\lambda} + \frac{1}{2\pi i \xi +\lambda} = \frac{(2\pi i \xi + \lambda) + (-2\pi i\xi + \lambda)}{(2\pi i \xi + \lambda) \cdot (-2\pi i\xi + \lambda)} =\frac{2\lambda}{4\pi^{2}\xi^{2} +\lambda^{2}}
\end{align*}
e quindi per ogni \(\xi \in \R\) si ha che \(\hat{f}(\xi)\neq \emptyset\).

Dunque \(f\) soddisfa le condizioni dei Teoremi \ref{teo:wt1} e \ref{teo:wt2}.
\item[{Potenziale di Laplace.}] Sia \(f(x)=\frac{1}{a^{2}+x^{2}}\) con \(a>0\).
\begin{equation*}
  \int_{\R} \bigg\lvert\frac{1}{a^{2}+x^{2}}\bigg\rvert\dif x =  \int_{\R} \frac{1}{a^{2}+x^{2}}\dif x = \left.\frac{\arctan(x/a)}{x}\right\rvert_{-\infty}^{\infty} = \frac{\pi}{a}<\infty
\end{equation*}
e dunque \(f \in L^{1}(\R)\). Inoltre \(f \in L^{2}(\R)\).

Si ha la seguente trasformata di Laplace:
\begin{align*}
  \hat{f}(\xi) &= \int_{\R}e^{-2\pi i \xi x} \cdot \frac{1}{a^{2}+x^{2}}\dif x = \frac{\pi}{a} e^{-2\pi a |\xi|}
\end{align*}
e quindi per ogni \(\xi \in \R\) si ha che \(\hat{f}(\xi)\neq \emptyset\).

Dunque \(f\) soddisfa le condizioni dei Teoremi \ref{teo:wt1} e \ref{teo:wt2}.

\item[{Gaussiana.}] Sia \(f(x) = e^{-ax^{2}}\), con \(a>0\).
\begin{align*}
  \int_{\R} |e^{-ax^{2}}| \dif x &= \int_{\R} e^{-ax^{2}}\dif x =\frac{1}{\sqrt{a}}\int_{\R} e^{-(\sqrt{a}x)^{2}}\sqrt{a}\dif x\\
  &= \frac{1}{\sqrt{a}} \int_{\R} e^{-u^{2}}\dif u = \frac{\sqrt{\pi}}{\sqrt{a}}<\infty
\end{align*}
e pertanto \(f \in L^{1}(\R)\) e \(f \in L_{2}^{\R}\).

Si calcola la trasformata di Laplace:
\begin{align*}
  \hat{f}(\xi) &= \int_{\R} e^{-2\pi i x \xi} e^{-ax^{2}}\dif x = \int_{\R} \exp\left(-(2\pi i \xi) x + -a x^{2}\right)\dif x\\
  &= \int_{\R} \exp\left(
      -\frac{\pi^{2}\xi^{2}}{a} +\frac{\pi^{2}\xi^{2}}{a} + - (2\pi i \xi) x + - a x^{2}
  \right)\dif x\\
  &= \exp\left(-\frac{\pi^{2}\xi^{2}}{a}\right) \int_{\R} \exp \left[
      -\left(-\frac{\pi^{2}\xi^{2}}{a} + 2\pi i \xi x + a x^{2} \right)
  \right]\dif x\\
  &= \exp\left(-\frac{\pi^{2}\xi^{2}}{a}\right) \int_{\R} \exp \left[
      -\left(\frac{i\pi\xi}{\sqrt{a}} + x \sqrt{a} \right)^{2}
  \right]\dif x\\
  &= \exp\left(-\frac{\pi^{2}\xi^{2}}{a}\right) \frac{\sqrt{\pi}}{\sqrt{a}}
\end{align*}
e quindi per ogni \(\xi \in \R\) si ha che \(\hat{f}(\xi)\neq \emptyset\).

Dunque \(f\) soddisfa le condizioni dei Teoremi \ref{teo:wt1} e \ref{teo:wt2}.
\end{description}
\chapter{Apprendimento con input unidimensionale}
\label{sec:orgd907c47}

\section{Risultati preliminari}
\label{sec:orge025d60}

\begin{prop}
Sia \(0=x_{0}<x_{1}<\dots<x_{N}=1\) una partizione di \([0,1]\), e sia\footnote{Con \(\chi_{A}\) si intende la \href{../../../../../org/roam/20250215160218-funzione_caratteristica.org}{funzione caratteristica} di \(A\).}
\begin{equation*}
c(x) = \sum_{i=0}^{N-1}\alpha_{i}\chi_{[x_{i},x_{i+1})}
\end{equation*}
Allora \(c(x)\) può essere scritto come combinazione lineare di \href{../../../../../org/roam/20250624161413-funzione_di_heaviside.org}{Funzioni di Heaviside}:
\begin{align*}
\chi_{[x_{i},x_{i+1})} &= H(x-x_{i})-H(x-x_{i+1})
;\\[1ex]
c(x) &= \sum_{i=0}^{N-1}\left(\alpha_{i}H(x-x_{i})-\alpha_{i} H(x-x_{i+1})\right)\\
&=\sum_{i=0}^{N} c_{i} H(x-x_{i}).
\end{align*}
\end{prop}
\begin{prop}
Se \(c(x)\) è come sopra, segue che la \href{../../../../../org/roam/20250701080039-derivata_distribuzioni.org}{derivata distribuzionale} di \(c\), \(c'(x)\), è\footnote{Con \(\delta(x)\) si intende la \href{../../../../../org/roam/20250625100133-delta_di_dirac.org}{Delta di Dirac}, e con \(\delta_{a}(x)\) si intende la Delta di Dirac centrata in \(a\).}
\begin{equation*}
c'(x) = \sum_{i=0}^{N} c_{i} H'(x-x_{i}) = \sum_{i=0}^{N} c_{i} \delta(x-x_{i}) = \sum_{i=0}^{N} c_{i} \delta_{x_{i}}(x)
\end{equation*}
\end{prop}
\begin{prop}
Sia \(\varphi:\R\to \R\) tale che
\begin{enumerate}
\item \(\varphi\) sia \href{../../../../../org/roam/20250203132953-funzione_monotona.org}{crescente};
\item \((\lim_{x\to+\infty}\varphi(x))-(\lim_{x\to-\infty}\varphi(x)) = 1\);
\item \(\varphi\) sia \href{../../../../../org/roam/20250627155431-funzione_derivabile.org}{derivabile}, con \(|\varphi'(x)|\) limitata.
\end{enumerate}

Sia \(\varphi_{\varepsilon}(x) \coloneqq \varphi(x/\varepsilon)\) e sia \(\mu_{\varepsilon}\) la misura con densità \(\varphi_{\varepsilon}'\):
\begin{equation*}
\dif\mu_{\varepsilon}(x) = \varphi_{\varepsilon}'(x)\dif x.
\end{equation*}
Allora\footnote{\(\delta\) è la \href{../../../../../org/roam/20250625100133-delta_di_dirac.org}{Delta di Dirac}} \(\varphi_{\varepsilon}\to \delta\) in senso debole, per \(\varepsilon\to 0\), ovvero, per ogni \(g \in C^{\infty}(\R)\)\footnote{Vedi \href{../../../../../org/roam/20250113125602-classe_c_di_una_funzione.org}{Classe C di una funzione}} a \href{../../../../../org/roam/20250701115005-supporto_di_una_funzione.org}{supporto} \href{../../../../../org/roam/20250103163701-spazio_topologico_compatto.org}{compatto}:
\begin{equation*}
\lim_{\varepsilon\to 0} \int_{\R} g(x)\dif \mu_{\varepsilon}(x) = \int_{\R} g(x)\delta(x)\dif x.
\end{equation*}
\end{prop}

\begin{proof}
Sia \(g \in C^{\infty}(\R)\) a supporto compatto.
\begin{align*}
\int_{\R}g(x)\dif\mu_{\varepsilon}(x) &= \int_{\R} g(x)\,\varphi_{\varepsilon}'(x)\dif x\\
&= \int_{\R} g(x)\,\varphi'\left(\frac{x}{\varepsilon}\right)\frac{1}{\varepsilon}\dif x\\
&= \int_{\R} g(\varepsilon y)\varphi'(y) \dif y & &\text{ponendo }y=\frac{x}{\varepsilon}
\end{align*}
Dunque, passando al limite:
\begin{align*}
\lim_{\varepsilon\to 0}\int_{\R}g(x)\dif\mu_{\varepsilon}(x) &= \lim_{\varepsilon\to {0}}\int_{\R}g(\varepsilon y)\varphi'(y)\dif y\\
&= \int_{\R}g(0)\varphi'(y)\dif y & &\text{per Teorema di Convergenza Dominata}\\
&= g(0) \int_{\R}\varphi'(y)\dif y = g(0) \cdot 1 & &\text{per l'ipotesi 2.}
\end{align*}
Siccome \(g(0)=\int_{\R}g(x)\delta(x)\dif x\), si ha la tesi.

È necessario ancora dimostrare che sia possibile applicare il teorema di convergenza dominata (questo si può fare solo perché \(g\) ha supporto compatto, e quindi l'integrale viene svolto su un dominio limitato):
\begin{equation*}
|g(\varepsilon y)\,\varphi'(y)| \le \norma{g}_{\infty}\,|\varphi'(y)| < M \in \R
\end{equation*}
poiché \(g\) ha supporto compatto ed è \(C^{\infty}\), mentre \(|\varphi'(y)|\) è limitato per ipotesi.
\end{proof}
\begin{thm}
Siano \((M,d), (N, \partial)\) due \href{../../../../../org/roam/20250301193511-spazio_metrico.org}{spazi metrici}, e sia \(f:M\to N\) \href{../../../../../org/roam/20250103103252-funzione_continua.org}{continua}.

Se \(M\) è \href{../../../../../org/roam/20250103163701-spazio_topologico_compatto.org}{compatto} allora \(f\) è \href{../../../../../org/roam/20250611135127-funzione_uniformemente_continua.org}{uniformemente continua}.
\end{thm}

\begin{cor}
In particolare, se \(g:[a,b]\to \R\) è continua, allora è uniformemente continua\footnote{Perché \([a,b]\) è compatto per il \href{../../../../../org/roam/20250701121640-teorema_di_heine_borel.org}{Teorema di Heine-Borel}.}.
\end{cor}
\section{Reti neurali che imparano funzioni continue}
\label{sec:org7553a6d}

\subsection{One Hidden Layer Perceptron Network}
\label{sec:orgafa4408}

\begin{thm}
Per ogni funzione \(g \in C[0,1]\)\textsuperscript{\ref{orgc8f3d78}} e per ogni \(\varepsilon>0\) esistono \(0=x_{0}<x_{1}<\dots<x_{N} = 1\) ed esistono \(c_{i} \in \R\) tali che, posta\footnote{Questa è una \href{../../../../../org/roam/20250701140621-funzione_costante_a_tratti.org}{Funzione costante a tratti}}
\begin{equation*}
c(x) = \sum_{i=0}^{N-1} c_{i} H(x-x_{i})
\end{equation*}
si ha che
\begin{equation*}
\forall x \in [0,1]:\quad |g(x)-c(x)|<\varepsilon
\end{equation*}
\end{thm}

\begin{proof}
Sia \(\varepsilon>0\) fissato. \href{../../../../../org/roam/20250701121640-teorema_di_heine_borel.org}{Siccome} \([0,1]\) è compatto, \href{../../../../../org/roam/20250630155208-funzione_reale_continua_su_un_compatto_e_uniformemente_continua.org}{allora} \(g\) è \href{../../../../../org/roam/20250611135127-funzione_uniformemente_continua.org}{uniformemente continua}. Sia \(\delta>0\) che soddisfi la condizione di uniforme continuità.

Sia \(0=x_{0}<\dots<x_{N}=1\) la equipartizione di \([0,1]\) tale che \(|x_{i+1}-x_{i}|<\delta\), e siano i \(c_{0},\dots,c_{N-1}\) tali che
\begin{align*}
g(x_{0}) &= c_{0} & c_{0}&= g(x_{0})\\
g(x_{1}) &= c_{0}+c_{1} & c_{1}&= g(x_{1})-g(x_{0})\\
&\vdots\\
g(x_{i}) &= c_{0}+c_{1}+\dots+c_{i}& c_{i}&= g(x_{i})-g(x_{i-1})\\
&\vdots\\
g(x_{N-1}) &= c_{0}+c_{1}+\dots+c_{N-1}& c_{N-1}&= g(x_{N-1})-g(x_{N-2}).
\end{align*}
Questo definisce la funzione \(c(x)\).

Sia ora \(u \in [0,1]\). Allora esiste \(k <N\) tale che \(u \in [x_{k},x_{k+1})\) e tale che \(|u-x_{k}|<\delta\). Si osservi quindi che
\begin{equation*}
H(u-x_{j}) = \begin{cases}
1 & j\le k\\
0 & j>k
\end{cases}
\end{equation*}
e pertanto \(c(u)\) vale:
\begin{equation*}
c(u) = \sum_{i=0}^{N-1} c_{i} \, H(u-x_{i}) = \sum_{i=0}^{k} c_{i} = g(x_{k}).
\end{equation*}
È possibile ora calcolare la distanza da \(g\):
\begin{align*}
|g(u)-c(u)| &= |g(u)-g(x_{k}) + g(x_{k}) - c(u)|\\
& \le |g(u)-g(x_{k})| + \cancel{|g(x_{k})-c(u)|} = |g(u)-g(x_{k})| <\varepsilon
\end{align*}
dove l'ultima condizione deriva dall'uniforme continuità di \(g\). Per l'arbitrarietà di \(u\) questo dimostra la tesi.
\end{proof}



\begin{oss}
La funzione \(c(x)\) di cui sopra è la funzione output di una \hyperref[sec:org18ff892]{rete neurale} con un layer nascosto:
\begin{itemize}
\item i pesi dall'input ai neuroni nascosti sono \(w_{i} = 1\);
\item ci sono \(N\) neuroni nascosti con funzione di attivazione \(H(x)\) e bias \(\theta_{i} = -x_{i}\);
\item i pesi dai neuroni nascosti al neurone di output sono i \(c_{i}\);
\item il neurone di output è lineare.
\end{itemize}
\end{oss}
\subsection{One Hidden Layer Sigmoid Network}
\label{sec:org00d548e}

\begin{thm}
Sia \(\varphi:\R\to \R\) tale che
\begin{enumerate}
\item \(\varphi\) sia \href{../../../../../org/roam/20250203132953-funzione_monotona.org}{crescente};
\item \((\lim_{x\to+\infty}\varphi(x))-(\lim_{x\to-\infty}\varphi(x)) = 1\);
\item \(\varphi\) sia \href{../../../../../org/roam/20250627155431-funzione_derivabile.org}{derivabile}, con \(|\varphi'(x)|\) limitata.
\end{enumerate}

Sia \(g \in C[0,1]\)\textsuperscript{\ref{orgc8f3d78}}. Allora per ogni \(\varepsilon>0\) esiste \(N \in \N\) ed esistono, per ogni \(i=0,\dots,N\) degli \(c_{i}, \theta_{i}, w \in \R\) tali che
\begin{equation*}
\forall  x \in [0,1]\qquad \bigg\lvert g(x) - \sum_{i=1}^{N} c_{i}\varphi(wx_{i} + \theta_{i})\bigg\rvert <\varepsilon
\end{equation*}
\end{thm}

\begin{oss}
In particolare, tutte le \hyperref[sec:orgeed12bd]{funzioni di attivazione} \href{../../../../../org/roam/20250625110110-funzione_sigmoidale.org}{sigmoidali} \(\varphi\) rispettano le ipotesi.
\end{oss}

\begin{proof}
Si divide la dimostrazione in fasi:
\begin{enumerate}
\item Approssimazione di \(g(x)\) con una \href{../../../../../org/roam/20250701140621-funzione_costante_a_tratti.org}{funzione costante a tratti} \(c(x)\) (di coefficienti arbitrariamente piccoli);
\item Costruzione di una versione ``smoother'' \(c_{\alpha}(x)\) di \(c(x)\) tramite la \href{../../../../../org/roam/20250703105424-prodotto_di_convoluzione.org}{convoluzione};
\item Aprrossimazione di \(c(x)\) con \(c_{\alpha}(x)\) (usando l'\href{../../../../../org/roam/20250630155139-misura_di_dirac_approssimata_da_misure_a_campana.org}{Approssimazione della misura di Dirac});
\item Approssimazione di \(g(x)\) con \(c_{\alpha}(x)\). (banale disuguaglianza triangolare)
\end{enumerate}

Vedi Theorem 8.3.1 di \cite{calinDeepLearningArchitectures2020}
\end{proof}
\subsection{One Hidden Layer ReLU Network}
\label{sec:orgf39fe71}

\begin{prop}
Sia \(g:[a,b]\to \R\) una funzione continua. Allora, per ogni \(\varepsilon>0\) esiste una partizione equidistante di \([a,b]\):
\begin{equation*}
a=x_{0}<x_{1}<\dots<x_{N} = b
\end{equation*}
tale che la \href{../../../../../org/roam/20250701140551-funzione_lineare_a_tratti.org}{funzione lineare a tratti} \(g_{\varepsilon} : [a,b]\to \R\) che passa per i punti \(\left(x_{i},g(x_{i})\right)\) per \(i=0,\dots,N\), soddisfa
\begin{equation*}
\forall x \in [a,b]:\quad |g(x)-g_{\varepsilon}(x)|<\varepsilon.
\end{equation*}
\end{prop}

\begin{proof}
\hphantom{Ciano}\par

\begin{description}
\item[{Parte 1.}] Sia \(\varepsilon>0\) fissato. \href{../../../../../org/roam/20250701121640-teorema_di_heine_borel.org}{Siccome} \([a,b]\) è compatto, \href{../../../../../org/roam/20250630155208-funzione_reale_continua_su_un_compatto_e_uniformemente_continua.org}{allora} \(g\) è \href{../../../../../org/roam/20250611135127-funzione_uniformemente_continua.org}{uniformemente continua}. Sia \(\delta>0\) che soddisfi la condizione di uniforme continuità per \(\varepsilon'=\varepsilon/2\).

Sia \(N\) sufficientemente grande affinché \(\frac{b-a}{N}<\delta\), e si definisca la partizione equidistante
\begin{equation*}
  x_{j} = a + \frac{b-a}{N}\, j
\end{equation*}
e la funzione lineare a tratti, per \(i=1,\dots,\):
\begin{equation*}
  g_{\varepsilon}(x) = g(x_{i-1}) +\frac{g(x_{i})-g(x_{i-1})}{x_{i}-x_{i-1}} (x-x_{i-1}),\qquad \forall x \in [x_{i-1},x_{i}]
\end{equation*}

\item[{Parte 2.}] Sia \(x \in [a,b]\) fissato. Allora \(x \in [x_{k-1},x_{k}]\) per qualche \(k\).
\begin{align*}
  |g(x)-g_{\varepsilon}(x)| &\le |g(x)-g(x_{k-1})| + |g(x_{k-1}) - g_{\varepsilon} (x)|\\
  &\le \frac{\varepsilon}{2} + |g(x_{k-1}) - g_{\varepsilon} (x)|\\
  &= \frac{\varepsilon}{2} + |g_{\varepsilon}(x_{k-1}) - g_{\varepsilon} (x)|\\
  &\le \frac{\varepsilon}{2}+ |g_{\varepsilon}(x_{k-1}) - g_{\varepsilon} (x_{k})|\\
  &= \frac{\varepsilon}{2}+ |g(x_{k-1}) - g (x_{k})| < \frac{\varepsilon}{2}+\frac{\varepsilon}{2} = \varepsilon\qedhere
\end{align*}
\end{description}
\end{proof}
\begin{thm}
Si consideri la \hyperref[sec:orgeed12bd]{funzione di attivazione} \(\operatorname{ReLU}(x) = x\,H(x)\)\footnote{Dove \(H(x)\) è la \href{../../../../../org/roam/20250624161413-funzione_di_heaviside.org}{Funzione di Heaviside}}. Allora per ogni \(g \in C[0,1]\)\textsuperscript{\ref{orgc8f3d78}} esiste \(N \in \N\) ed esistono, per \(i=0,\dots,N-1\) degli \(\alpha_{i}, \theta_{i}, \beta \in \R\) tali che, detta
\begin{equation*}
G(x) \coloneqq \beta+\sum_{i=0}^{N-1} \alpha_{i}\operatorname{ReLU}(x+\theta_{i})
\end{equation*}
si ha che
\begin{equation*}
\forall x \in[0,1]\qquad |g(x)-G(x)|<\varepsilon.
\end{equation*}
\end{thm}
\subsection{One Hidden Layer softplus Network}
\label{sec:org63c2ab7}

\begin{lem}
La \hyperref[sec:orgeed12bd]{funzione di attivazione} \emph{\hyperref[sec:orga531b0f]{softplus}} \(\operatorname{sp}(x)\) è data dalla \href{../../../../../org/roam/20250703105424-prodotto_di_convoluzione.org}{convoluzione}:\footnote{Vedi la \hyperref[sec:org8e4299d]{funzione \(\operatorname{ReLU}\)}}
\begin{equation*}
\operatorname{sp}(x) = \convolution{\operatorname{ReLU}}{K}(x) = \int_{-\infty}^{+\infty} \operatorname{ReLU}(\tau) K(x-\tau)\dif\tau
\end{equation*}
dove \(K(x) = \frac{1}{(1+e^{x})(1+e^{-x})} = \sigma'(x)\)\footnote{Dove \(\sigma(x)\) è la \hyperref[sec:org1d40102]{Funzione Logistica}.}.
\end{lem}

\begin{lem}
Sia \(K(x) \coloneqq \frac{1}{(1+e^{x})(1+e^{-x})}\). Allora
\begin{enumerate}
\item \(K(x)\) è una \href{../../../../../org/roam/20250703142106-densita_di_probabilita.org}{densità di probabilità} simmetrica;
\item Sia \(K_{\alpha}\coloneqq \frac{1}{\alpha}K(x/\alpha)\) e si consideri la misura \(\mu_{\alpha}\) tale che
\begin{equation*}
 \dif \mu_{\alpha} \coloneqq K_{\alpha}(x)\dif x.
\end{equation*}
Allora \(\int_{-\infty}^{+\infty} K_{\alpha}(x)\dif x= 1\) e \(\mu_{\alpha}\to \delta\)\footnote{\(\delta\) è la \href{../../../../../org/roam/20250625100133-delta_di_dirac.org}{Delta di Dirac}} in senso debole.
\end{enumerate}
\end{lem}

\begin{lem}
Si definisca ora la \hyperref[sec:orga531b0f]{funzione softplus} scalata:
\begin{equation*}
\varphi_{\alpha}(x) \coloneqq \alpha \operatorname{sp}\left(\frac{x}{\alpha}\right) = \alpha\ln(1
+ e^{x/\alpha})
\end{equation*}
Si ha che
\begin{equation*}
\varphi_{\alpha}''(x) = \frac{1}{\alpha}\operatorname{sp}''(x/\alpha) = \frac{1}{\alpha}K(x/\alpha)=K_{\alpha}(x)
\end{equation*}
poiché \(\operatorname{sp}'(x)=\sigma(x)\) e \(\sigma'(x) = K(x)\).
\end{lem}

\begin{lem}
Si consideri la \href{../../../../../org/roam/20250703105424-prodotto_di_convoluzione.org}{convoluzione} \(G_{\alpha}\coloneqq G\conv K_{\alpha}\), dove
\begin{equation*}
G(x) = \sum_{j=0}^{N-1} \alpha_{j} \operatorname{ReLU}(x-x_{j})+\beta
\end{equation*}
per qualche \(N \in \N\), \(\alpha_{j}, x_{j}, \beta \in \R\).

Allora esistono \(c_{j}, w, \theta_{j} \in \R\), che dipendono dagli \(\alpha_{j}, x_{j}\) tali che
\begin{equation*}
G_{\alpha}(x) = \sum_{j=0}^{N-1}c_{j}\operatorname{sp}(wx-\theta_{j}) + \beta
\end{equation*}
\end{lem}

\begin{lem}
Per ogni \(\varepsilon>0\) esiste \(\eta>0\) tale per cui, se \(\alpha<\eta\) allora
\begin{equation*}
\forall x \in [0,1]\qquad |G(x)-G_{\alpha}(x)|<\varepsilon
\end{equation*}
ovvero \(G_{\alpha}\) \href{../../../../../org/roam/20250629105745-convergenza_uniforme.org}{converge uniformemente} a \(G\) su \([0,1]\).
\end{lem}

\begin{proof}
Banale applicazione del Teorema del Dini dopo aver notato che
\begin{equation*}
\varphi_{\alpha}(x)\to \operatorname{ReLU}(x).
\end{equation*}
puntualmente, e \(\varphi_{\alpha}<\varphi_{\beta}\) se \(\alpha>\beta\).
\end{proof}

\begin{thm}
Sia \(g \in C[0,1]\)\textsuperscript{\ref{orgc8f3d78}}. Allora per ogni \(\varepsilon>0\) esiste \(N \in \N\) ed esistono, per ogni \(i=0,\dots,N-1\), degli \(c_{j},w,\theta_{j},\beta \in \R\) tali che
\begin{equation*}
\forall x \in [0,1]\qquad \bigg\lvert g(x)-\sum_{j=0}^{N-1}c_{j}\operatorname{sp}(wx-\theta_{j})-\beta\bigg\rvert<\varepsilon.
\end{equation*}
\end{thm}
\chapter{Universal Approximation}
\label{sec:orgd7d8db1}



\part{Sirovich}
\end{document}
